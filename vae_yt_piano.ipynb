{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8cf725e",
   "metadata": {},
   "source": [
    "# Training a Conditional Variational Autoencoder to generate music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7dbf7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.2 (SDL 2.0.18, Python 3.9.7)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pretty_midi\n",
    "import pypianoroll\n",
    "\n",
    "import scipy.sparse\n",
    "from scipy.sparse import coo_matrix, save_npz, load_npz\n",
    "\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, Input, Sequential\n",
    "from tensorflow.keras.layers import InputLayer, Flatten, Reshape\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Conv2DTranspose\n",
    "from tensorflow.keras.layers import Conv1D, Conv1DTranspose\n",
    "from tensorflow.keras.layers import Conv3D, Conv3DTranspose\n",
    "from tensorflow.keras.layers import ConvLSTM1D\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import GRU, LSTM\n",
    "from tensorflow.keras.layers import Lambda\n",
    "from tensorflow.keras.layers import Concatenate, concatenate\n",
    "from tensorflow.keras.layers import BatchNormalization as BatchNorm\n",
    "from tensorflow.keras.layers import ReLU as Relu\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "import math\n",
    "\n",
    "import datetime\n",
    "from IPython import display\n",
    "\n",
    "import pygame\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30bc64c",
   "metadata": {},
   "source": [
    "## Preparing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a95f6f",
   "metadata": {},
   "source": [
    "The data used in this notebook is the Youtube Piano dataset, which consists of about 10,800 MIDI files with a single track of a piano. Prior to the execution of this notebook, I converted these MIDI files into piano rolls (sampled with frequency 1/100), which were then saved as COO scipy sparse matrices. Although the COO matrices take up more space than the MIDI files, I found that it was better for performance to save the piano rolls and then load them into memory, rather than load the MIDI files into memory and then have to constantly convert them to piano rolls while training, or to get training samples from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b71b6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT_DIR = 'C:/_local/data_sets/audio/youtube_piano/'\n",
    "\n",
    "MIDI_FILES_DIR = DATA_ROOT_DIR + 'midis/'\n",
    "PIANO_ROLL_DIR = DATA_ROOT_DIR + 'piano_rolls/'\n",
    "PIANO_ROLL_MASKS_DIR = DATA_ROOT_DIR + 'piano_roll_masks/'\n",
    "\n",
    "MODELS_ROOT_DIR = 'C:/_local/py/yt_piano_music_gen/models/'\n",
    "CVAE_DIR = MODELS_ROOT_DIR + 'cvae/'\n",
    "NEW_VAE_DIR = MODELS_ROOT_DIR + 'new_cvae/'\n",
    "MELODY_MODELS_DIR = MODELS_ROOT_DIR + 'melody_models/'\n",
    "CCVAE_MODELS_DIR = MODELS_ROOT_DIR + 'cond_conv_vae/'\n",
    "MELODY_PREDICTION_MODELS_DIR = MODELS_ROOT_DIR + 'melody_predictor/'\n",
    "\n",
    "OUTPUTS_ROOT_DIR = 'C:/_local/py/yt_piano_music_gen/outputs/'\n",
    "NOTE_OUTPUT_DIR = OUTPUTS_ROOT_DIR + 'notes/'\n",
    "SIMPLE_SEQUENCE_OUTPUT_DIR = OUTPUTS_ROOT_DIR + 'simple_sequences/'\n",
    "MELODY_OUTPUT_DIR = OUTPUTS_ROOT_DIR + 'melodies/'\n",
    "TEMP_OUTPUT_PATH = OUTPUTS_ROOT_DIR + 'temp.mid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d33c5f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIANO_ROLL_PATHS = np.array([PIANO_ROLL_DIR+f for f in os.listdir(PIANO_ROLL_DIR)])\n",
    "MIDI_FILE_PATHS = np.array([MIDI_FILES_DIR+f for f in os.listdir(MIDI_FILES_DIR)])\n",
    "CURRENT_NUM_PIANO_ROLLS = len(PIANO_ROLL_PATHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b5e2131",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_SAMPLES_SUBSET = 1200 # <-- Number of piano rolls to randomly sample for training/testing\n",
    "\n",
    "MELODY_INPUT_TIME_S = 6.4 # <-- Length of inputs for melody model in seconds\n",
    "MELODY_INPUT_NUM_SEQUENCES = int(MELODY_INPUT_TIME_S * 100 // 64) # <-- Number of SEQUENCE_LENGTH sequences as input for melody model\n",
    "MELODY_BUFFER_S = .6 # <-- Extra time for masking piano rolls\n",
    "\n",
    "TRIM_START = True # <-- Only consider sequences after first note played\n",
    "TRIM_END = True # <-- Only consider sequences before last note played\n",
    "\n",
    "SEQUENCE_LENGTH = 32 # <-- length of input / target sequences in 1/100 seconds\n",
    "LATENT_DIM = 128\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5a16715",
   "metadata": {},
   "outputs": [],
   "source": [
    "MASK_TIME_STRING = '-'.join(str(round(MELODY_INPUT_TIME_S+MELODY_BUFFER_S, 1)).split('.'))\n",
    "PIANO_ROLLS_MASK_PATH = PIANO_ROLL_MASKS_DIR + f'melody_{MASK_TIME_STRING}_s_{CURRENT_NUM_PIANO_ROLLS}'\n",
    "PIANO_ROLLS_MASK = None\n",
    "MASKED_PIANO_ROLLS_PATHS = None\n",
    "\n",
    "def get_if_piano_roll_long_enough(piano_roll_path):\n",
    "    \n",
    "    pr = load_npz(piano_roll_path)\n",
    "    return pr.col[-1] - pr.col[0] > MELODY_INPUT_TIME_S + MELODY_BUFFER_S\n",
    "\n",
    "if not os.path.isfile(PIANO_ROLLS_MASK_PATH):\n",
    "    \n",
    "    mask = [get_if_piano_roll_long_enough(prp) for prp in PIANO_ROLL_PATHS]\n",
    "    \n",
    "    with open(PIANO_ROLLS_MASK_PATH, 'wb') as f:\n",
    "        pickle.dump(mask, f)\n",
    "        \n",
    "with open(PIANO_ROLLS_MASK_PATH, 'rb') as f:\n",
    "    PIANO_ROLLS_MASK = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f5e5cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions for converting scipy-sparse matrices to tf.sparse.SparseTensor\n",
    "def scipy_sparse_to_sparse_tensor(scipy_sparse):\n",
    "    \n",
    "    indices = np.mat([scipy_sparse.row, scipy_sparse.col]).transpose()\n",
    "    return tf.cast(tf.sparse.SparseTensor(indices, \n",
    "                                          scipy_sparse.data, \n",
    "                                          scipy_sparse.shape),\n",
    "                    dtype=tf.float32\n",
    "                   )\n",
    "\n",
    "def list_of_scipy_sparse_to_list_sparse_tensor(list_scipy_sparse):\n",
    "    return [scipy_sparse_to_sparse_tensor(s) for s in list_scipy_sparse]\n",
    "\n",
    "def piano_roll_path_to_sparse_tensor(piano_roll_path):\n",
    "    \n",
    "    s = load_npz(piano_roll_path)   \n",
    "    return scipy_sparse_to_sparse_tensor(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68ab481f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of piano rolls: 2000\n",
      "total size of piano rolls: 0.582 gb\n",
      "\n",
      "number of piano rolls in subset: 1403\n",
      "total size of piano rolls: 0.417 gb\n"
     ]
    }
   ],
   "source": [
    "print(f'total number of piano rolls: {len(PIANO_ROLL_PATHS)}')\n",
    "\n",
    "piano_rolls_size_gb = 0\n",
    "for pr in PIANO_ROLL_PATHS:                      # kb   # mb   # gb\n",
    "    piano_rolls_size_gb += os.path.getsize(pr) / 1000 / 1000 / 1000\n",
    "    \n",
    "print(f'total size of piano rolls: {round(piano_rolls_size_gb, 3)} gb\\n')\n",
    "\n",
    "\n",
    "\n",
    "PIANO_ROLL_PATHS_MASKED = PIANO_ROLL_PATHS[PIANO_ROLLS_MASK]\n",
    "\n",
    "print(f'number of piano rolls in subset: {len(PIANO_ROLL_PATHS_MASKED)}')\n",
    "\n",
    "piano_rolls_size_gb = 0\n",
    "for pr in PIANO_ROLL_PATHS_MASKED:                      # kb   # mb   # gb\n",
    "    piano_rolls_size_gb += os.path.getsize(pr) / 1000 / 1000 / 1000\n",
    "    \n",
    "print(f'total size of piano rolls: {round(piano_rolls_size_gb, 3)} gb')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040bbff6",
   "metadata": {},
   "source": [
    "The following cell randomly selects NUMBER_OF_PIANO_ROLLS_SUBSET piano rolls to train and test our CVAE on. Since each piano roll has at least a few thousand time-steps (so at least a few thousand sequences for training), this should be enough data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "993925d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of piano rolls: 500\n",
      "total size of piano rolls: 0.15 gb\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "PIANO_ROLL_PATHS_SUBSET = np.random.choice(PIANO_ROLL_PATHS_MASKED, size=500, replace=False)\n",
    "\n",
    "print(f'Number of piano rolls: {len(PIANO_ROLL_PATHS_SUBSET)}')\n",
    "\n",
    "piano_rolls_size_gb = 0\n",
    "for pr in PIANO_ROLL_PATHS_SUBSET:               # kb   # mb   # gb\n",
    "    piano_rolls_size_gb += os.path.getsize(pr) / 1000 / 1000 / 1000\n",
    "    \n",
    "print(f'total size of piano rolls: {round(piano_rolls_size_gb, 3)} gb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fea3676b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_piano_rolls_nonzero_mean_std(piano_roll_paths):\n",
    "    \n",
    "    nz_sums = []\n",
    "    total_nnz = 0\n",
    "    \n",
    "    sparse = None\n",
    "        \n",
    "    for prp in piano_roll_paths:\n",
    "        sparse = load_npz(prp)\n",
    "        nz_sums.append(sparse.data.sum())\n",
    "        total_nnz += sparse.nnz\n",
    "    \n",
    "    mean = (np.array(nz_sums) / total_nnz).sum()\n",
    "    \n",
    "    def get_std_disc(x, u):\n",
    "        return (((x - u)**2)/total_nnz).sum()\n",
    "    \n",
    "    nz_std_disc_sum = 0\n",
    "    for prp in piano_roll_paths:\n",
    "        sparse = load_npz(prp)\n",
    "        nz_std_disc_sum += get_std_disc(sparse.data, mean)\n",
    "    \n",
    "    std = nz_std_disc_sum ** (.5)\n",
    "    \n",
    "    return mean, std\n",
    "\n",
    "def get_sparse_mats_mean_std(sparse_matrices):\n",
    "    \n",
    "    sums = []\n",
    "    total_size = 0\n",
    "        \n",
    "    for s in sparse_matrices:\n",
    "        sums.append(s.A.sum())\n",
    "        total_size += s.shape[0]*s.shape[1]\n",
    "    \n",
    "    mean = (np.array(sums) / total_size).sum()\n",
    "    \n",
    "    def get_std_disc(x, u):\n",
    "        return (((x - u)**2)/total_size).sum()\n",
    "    \n",
    "    std_disc_sum = 0\n",
    "    for s in sparse_matrices:\n",
    "        std_disc_sum += get_std_disc(s.A.reshape(-1,1), mean)\n",
    "    \n",
    "    std = nz_std_disc_sum ** (.5)\n",
    "    \n",
    "    return mean, std\n",
    "\n",
    "def normal_sparse_matrix(sparse_matrix, mean, std, inplace):\n",
    "    \n",
    "    if inplace:\n",
    "        sparse_matrix.data = (sparse_matrix.data - mean) / std\n",
    "    else:\n",
    "        sparse_matrix_new = sparse_matrix\n",
    "        sparse_matrix_new.data = (sparse_matrix_new.data - mean) / std\n",
    "        return sparse_matrix_new\n",
    "\n",
    "def normalize_list_of_sparse_matrices(sparse_matrices):\n",
    "    \n",
    "    mean, std = get_sparse_mats_nonzero_mean_std(sparse_matrices)\n",
    "    \n",
    "    for s in sparse_matrices:    \n",
    "        s.data = (s.data - mean) / std\n",
    "        \n",
    "    return sparse_matrices\n",
    "\n",
    "def piano_roll_paths_to_norm_sparse_tensors(piano_roll_paths):\n",
    "    \n",
    "    mean, std = get_piano_rolls_nonzero_mean_std(piano_roll_paths)\n",
    "    \n",
    "    sparse_tensors = []\n",
    "    for prp in piano_roll_paths:\n",
    "        \n",
    "        sparse_matrix = load_npz(prp)\n",
    "        normal_sparse_matrix(sparse_matrix, mean, std, True)\n",
    "        sparse_tensors.append(scipy_sparse_to_sparse_tensor(sparse_matrix))\n",
    "        \n",
    "    return sparse_tensors\n",
    "\n",
    "def piano_roll_paths_to_scaled_sparse_tensors(piano_roll_paths):\n",
    "    \n",
    "    sparse_tensors = []\n",
    "    for prp in piano_roll_paths:\n",
    "        \n",
    "        sparse_matrix = load_npz(prp)\n",
    "        sparse_matrix.data[sparse_matrix.data > 127] = 127.\n",
    "        sparse_matrix.data = sparse_matrix.data / 127.\n",
    "        \n",
    "        sparse_tensors.append(scipy_sparse_to_sparse_tensor(sparse_matrix))\n",
    "        \n",
    "    return sparse_tensors\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e08d974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now load the NUMBER_OF_PIANO_ROLLS_SUBSET piano rolls into memory\n",
    "\n",
    "PIANO_ROLLS = np.array(piano_roll_paths_to_scaled_sparse_tensors(PIANO_ROLL_PATHS_SUBSET))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8f61b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIANO_ROLLS_TRAIN, PIANO_ROLLS_TEST = train_test_split(PIANO_ROLLS, random_state=2, test_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dacfec7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training piano rolls: 400\n",
      "testing piano rolls: 100\n"
     ]
    }
   ],
   "source": [
    "print(f'training piano rolls: {PIANO_ROLLS_TRAIN.shape[0]}')\n",
    "print(f'testing piano rolls: {PIANO_ROLLS_TEST.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386741ae",
   "metadata": {},
   "source": [
    "## Piano Roll helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "615d8517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def piano_roll_to_pretty_midi(pr, ctrl=None, constant_tempo=None, constant_velocity=100):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    pr    : NumPy array of size (t, 128)\n",
    "    ctrl  : list of length t with - \n",
    "              - binary values 0 and 1, where 1 denotes a note onset (for monophonic)\n",
    "              - 0 and pitch values, where pitch values denote a note onset (for polyphonic)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pm    : `pretty_midi.PrettyMIDI` object\n",
    "        The converted :class:`pretty_midi.PrettyMIDI` instance.\n",
    "    '''\n",
    "    beat_resolution = 4\n",
    "    pm = pretty_midi.PrettyMIDI()\n",
    "\n",
    "    if constant_tempo is None:\n",
    "        constant_tempo = 128\n",
    "    time_step_size = 60. / constant_tempo / beat_resolution\n",
    "\n",
    "    instrument = pretty_midi.Instrument(program=0, is_drum=False, name=\"test\")\n",
    "    clipped = pr.astype(np.uint8)\n",
    "    binarized = (clipped > 0)\n",
    "    padded = np.pad(binarized, ((1, 1), (0, 0)), 'constant')\n",
    "    diff = np.diff(padded.astype(np.int8), axis=0)\n",
    "    \n",
    "    positives = np.nonzero((diff > 0).T)\n",
    "    pitches = positives[0]\n",
    "    note_ons = positives[1]\n",
    "    note_on_times = time_step_size * note_ons\n",
    "    note_offs = np.nonzero((diff < 0).T)[1]\n",
    "    note_off_times = time_step_size * note_offs\n",
    "    \n",
    "    if ctrl is None:\n",
    "        for idx, pitch in enumerate(pitches):\n",
    "            velocity = np.mean(clipped[note_ons[idx]:note_offs[idx], pitch])\n",
    "            note = pretty_midi.Note(\n",
    "                velocity=int(velocity), pitch=pitch,\n",
    "                start=note_on_times[idx], end=note_off_times[idx])\n",
    "            instrument.notes.append(note)\n",
    "    \n",
    "    else:\n",
    "        pairs = []\n",
    "        for idx, pitch in enumerate(pitches):\n",
    "            note_on, note_off = note_ons[idx], note_offs[idx]\n",
    "            true_ons = ctrl[note_ons[idx]:note_offs[idx]]\n",
    "            on_idx = [i for i in range(len(true_ons)) if true_ons[i] == 1]  # if polyphonic, change 1 to pitch value\n",
    "            on_idx.pop(0)  # remove 1st onset token\n",
    "\n",
    "            cur_note_on = note_on\n",
    "            while on_idx:\n",
    "                cur_note_off = note_on + on_idx[0]\n",
    "                pairs.append((pitch, cur_note_on, cur_note_off))\n",
    "                cur_note_on = cur_note_off\n",
    "                on_idx.pop(0)\n",
    "            pairs.append((pitch, cur_note_on, note_off))   \n",
    "        \n",
    "        for idx, p in enumerate(pairs):\n",
    "            pitch, start, end = p\n",
    "            velocity = np.mean(clipped[start:end, pitch])\n",
    "            note = pretty_midi.Note(\n",
    "                velocity=int(velocity), pitch=pitch,\n",
    "                start=start*time_step_size, end=end*time_step_size)\n",
    "            instrument.notes.append(note)\n",
    "\n",
    "    instrument.notes.sort(key=lambda x: x.start)\n",
    "    pm.instruments.append(instrument)\n",
    "    \n",
    "    return pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "703284b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def piano_roll_to_pretty_midi(piano_roll, fs=100):\n",
    "    \n",
    "    notes, frames = piano_roll.shape\n",
    "    pm = pretty_midi.PrettyMIDI()\n",
    "    instrument = pretty_midi.Instrument(program=0)\n",
    "\n",
    "    # pad 1 column of zeros so we can acknowledge inital and ending events\n",
    "    piano_roll = np.pad(piano_roll, [(0, 0), (1, 1)], 'constant')\n",
    "\n",
    "    # use changes in velocities to find note on / note off events\n",
    "    velocity_changes = np.nonzero(np.diff(piano_roll).T)\n",
    "\n",
    "    # keep track on velocities and note on times\n",
    "    prev_velocities = np.zeros(notes, dtype=int)\n",
    "    note_on_time = np.zeros(notes)\n",
    "\n",
    "    for time, note in zip(*velocity_changes):\n",
    "        # use time + 1 because of padding above\n",
    "        velocity = piano_roll[note, time + 1]\n",
    "        time = time / fs\n",
    "        if velocity > 0:\n",
    "            if prev_velocities[note] == 0:\n",
    "                note_on_time[note] = time\n",
    "                prev_velocities[note] = velocity\n",
    "        else:\n",
    "            pm_note = pretty_midi.Note(\n",
    "                velocity=prev_velocities[note],\n",
    "                pitch=note,\n",
    "                start=note_on_time[note],\n",
    "                end=time)\n",
    "            instrument.notes.append(pm_note)\n",
    "            prev_velocities[note] = 0\n",
    "    pm.instruments.append(instrument)\n",
    "    return pm    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef5c5351",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_piano_roll(piano_roll, buffer_time=0, threshold=.3, temp_path=TEMP_OUTPUT_PATH):\n",
    "    \n",
    "    if isinstance(piano_roll, tf.sparse.SparseTensor):\n",
    "        piano_roll = tf.sparse.to_dense(piano_roll).numpy()\n",
    "    \n",
    "    if piano_roll.max() < 1.5:\n",
    "        piano_roll *= 127.\n",
    "    \n",
    "    piano_roll[piano_roll > 127] = 126\n",
    "    #piano_roll[piano_roll <= 127*threshold] = 0\n",
    "    piano_roll = piano_roll.astype('uint8').round()\n",
    "            \n",
    "    midi = piano_roll_to_pretty_midi(piano_roll)\n",
    "    midi.write(temp_path)\n",
    "    \n",
    "    sleep_time = buffer_time + piano_roll.shape[-1] / 100\n",
    "    \n",
    "    pygame.mixer.init()\n",
    "    pygame.mixer.music.load(temp_path)\n",
    "    \n",
    "    pygame.mixer.music.play()\n",
    "    time.sleep(sleep_time)\n",
    "    \n",
    "    pygame.mixer.music.stop()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5c629fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_samples_from_batch(batch, number_of_samples=None, shuffle=None, temp_path=TEMP_OUTPUT_PATH):\n",
    "    \n",
    "    if batch.max() <= 2:\n",
    "        batch *= 127.\n",
    "        \n",
    "    batch[batch > 127] = 127\n",
    "    batch[batch < 0] = 0\n",
    "    batch = batch.round().astype('uint8')\n",
    "    batch = batch.squeeze()\n",
    "        \n",
    "    if number_of_samples is None:\n",
    "        number_of_samples = batch.shape[0]\n",
    "        \n",
    "    if shuffle is None:\n",
    "        shuffle = False\n",
    "        \n",
    "    if shuffle:\n",
    "        steps = np.random.choice(np.arange(batch.shape[0]), size=number_of_samples, replace=False)\n",
    "        steps = sorted(steps)\n",
    "        \n",
    "    else: \n",
    "        steps = range(number_of_samples)\n",
    "        \n",
    "    sleep_time = batch.shape[-1] / 100\n",
    "        \n",
    "    for s in steps:\n",
    "        \n",
    "        display.clear_output(wait=False)\n",
    "        print(f'sample # {s}')\n",
    "        \n",
    "        pr = batch[s].squeeze()\n",
    "        midi = piano_roll_to_pretty_midi(pr)\n",
    "        \n",
    "        midi.write(temp_path)\n",
    "\n",
    "        pygame.mixer.init()\n",
    "        pygame.mixer.music.load(temp_path)\n",
    "        pygame.mixer.music.play()\n",
    "\n",
    "        time.sleep(sleep_time)\n",
    "        pygame.mixer.music.stop()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d93576df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_inputs_and_outputs(inputs, outputs, number_of_samples, shuffle):\n",
    "    \n",
    "    # Inputs is ndarray shaped batch_size x number_of_melodies x 128 x sequence_length\n",
    "    # Outputs is ndarray batch_size x 128 x sequence_length\n",
    "        \n",
    "    if shuffle:\n",
    "        possible_sample_indices = range(outputs.shape[0])\n",
    "        steps = np.random.choice(possible_sample_indices, size=number_of_samples, replace=False)\n",
    "        \n",
    "    else: \n",
    "        steps = range(number_of_samples)\n",
    "        \n",
    "    input_time = inputs.shape[1]*inputs.shape[-1] / 100\n",
    "    output_time = outputs.shape[2] / 100\n",
    "    total_time = input_time + output_time + .3\n",
    "    \n",
    "    def np_unstack(array):\n",
    "        nps = [array[b] for b in range(array.shape[0])]\n",
    "        return nps\n",
    "    \n",
    "    for s in steps:\n",
    "        \n",
    "        display.clear_output(wait=False)\n",
    "        print(f'sample # {s}')\n",
    "        \n",
    "        sample_input = inputs[s]\n",
    "        sample_input = np_unstack(sample_input)\n",
    "        sample_input.append(outputs[s])\n",
    "        \n",
    "        pr = np.concatenate(sample_input, axis=-1)\n",
    "        midi = piano_roll_to_pretty_midi(pr)\n",
    "                \n",
    "        midi.write(TEMP_OUTPUT_PATH)\n",
    "\n",
    "        pygame.mixer.init()\n",
    "        pygame.mixer.music.load(TEMP_OUTPUT_PATH)\n",
    "        \n",
    "        pygame.mixer.music.play()\n",
    "        time.sleep(total_time)\n",
    "        \n",
    "        pygame.mixer.music.stop() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1aec29",
   "metadata": {},
   "source": [
    "## Creating TF Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "b53be288",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoteTargetGenerator:    \n",
    "    \n",
    "    def __init__(self, sparse_tensors, yield_target, sequence_length, seed=None):\n",
    "        \n",
    "        self.sparse_tensors = sparse_tensors\n",
    "        self.num_tensors = len(self.sparse_tensors)\n",
    "        self.yield_target = yield_target\n",
    "        self.sequence_length = sequence_length\n",
    "        self.seed = seed\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "        \n",
    "        while True:\n",
    "            \n",
    "            sparse_tensor = np.random.choice(self.sparse_tensors, size=1)[0]\n",
    "        \n",
    "            last_start = (sparse_tensor.shape[1] - 2 * self.sequence_length - 3)\n",
    "            note_start = np.random.choice(np.arange(last_start), size=1)[0]\n",
    "\n",
    "            note = tf.sparse.slice(sparse_tensor,\n",
    "                                   start=[0, note_start],\n",
    "                                   size=[128, self.sequence_length]\n",
    "                                  )\n",
    "            \n",
    "            #yield tf.sparse.to_dense(note)\n",
    "            yield tf.expand_dims(tf.sparse.to_dense(note), axis=-1)\n",
    "            \n",
    "            \n",
    "    def __call__(self):\n",
    "        return self.__iter__()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "32d6b5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SIZE_OF_GENERATORS = 50\n",
    "\n",
    "train_sub_generators = [NoteTargetGenerator(PIANO_ROLLS_TRAIN[i*DATA_SIZE_OF_GENERATORS:(i+1)*DATA_SIZE_OF_GENERATORS], False, SEQUENCE_LENGTH, i)\n",
    "                        for i in range(1 + PIANO_ROLLS_TRAIN.shape[0] // DATA_SIZE_OF_GENERATORS)\n",
    "                        if i*DATA_SIZE_OF_GENERATORS < PIANO_ROLLS_TRAIN.shape[0]]\n",
    "\n",
    "test_sub_generators = [NoteTargetGenerator(PIANO_ROLLS_TEST[i*DATA_SIZE_OF_GENERATORS:(i+1)*DATA_SIZE_OF_GENERATORS], False, SEQUENCE_LENGTH, i)\n",
    "                        for i in range(1 + PIANO_ROLLS_TEST.shape[0] // DATA_SIZE_OF_GENERATORS)\n",
    "                        if i*DATA_SIZE_OF_GENERATORS < PIANO_ROLLS_TEST.shape[0]]\n",
    "\n",
    "cvae_gen_output_signature = tf.TensorSpec(shape=(128, SEQUENCE_LENGTH, 1))\n",
    "\n",
    "def get_sub_dataset(sub_generator, spec, batch_size, prefetch_size):\n",
    "    \n",
    "    return (tf.data.Dataset\n",
    "            .from_generator(sub_generator, output_signature=spec)\n",
    "            .batch(batch_size, drop_remainder=True)\n",
    "            .prefetch(prefetch_size)\n",
    "           )\n",
    "\n",
    "cvae_train_sub_datasets = [get_sub_dataset(g, cvae_gen_output_signature, BATCH_SIZE, 10)\n",
    "                           for g in train_sub_generators]\n",
    "cvae_test_sub_datasets = [get_sub_dataset(g, cvae_gen_output_signature, BATCH_SIZE, 10)\n",
    "                           for g in test_sub_generators]\n",
    "\n",
    "cvae_train_dataset = tf.data.Dataset.sample_from_datasets(cvae_train_sub_datasets).prefetch(64)\n",
    "cvae_test_dataset = tf.data.Dataset.sample_from_datasets(cvae_test_sub_datasets).prefetch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "883f24d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 107 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "np.random.seed(333)\n",
    "sample_input = None\n",
    "for x in cvae_test_dataset.take(1):\n",
    "    sample_input = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "18b70ca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([32, 128, 32, 1])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "bc269856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108.00000149011612"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_input.numpy().max()*127."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b31eb03",
   "metadata": {},
   "source": [
    "## Training the CVAE to produce notes\n",
    "\n",
    "First, we train the CVAE to learn the distribution of pianoroll values which correspond to notes/chords, and how to generate them. It is trained on batches of 128 x SEQUENCE_LENGTH tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea9cea0",
   "metadata": {},
   "source": [
    "### CVAE Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e0426305",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, latent_dim, sequence_length):\n",
    "        super(CVAE, self).__init__()\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "        #encoder_input = Input(shape=(128, self.sequence_length), name='encoder_input')\n",
    "        #encoder_reshape = Reshape(target_shape=(128, self.sequence_length, 1), name='encoder_reshape')(encoder_input)\n",
    "        \n",
    "        encoder_input = Input(shape=(128, self.sequence_length, 1), name='encoder_input')\n",
    "        \n",
    "        encoder_conv_1 = Conv2D(filters=64,kernel_size=(4, 4), strides=(4, 4), padding='valid', activation='relu',name='encoder_conv2d_1')(encoder_input)\n",
    "        encoder_conv_2 = Conv2D(filters=128, kernel_size=(4, 4), strides=(4, 4), padding='valid', activation='relu', name='encoder_conv2d_2')(encoder_conv_1)\n",
    "        encoder_conv_3 = Conv2D(filters=256, kernel_size=(8, 4), strides=(8, 4), padding='valid', activation='relu', name='encoder_conv2d_3')(encoder_conv_2)\n",
    "        encoder_flatten = Flatten(name='encoder_flatten')(encoder_conv_3)\n",
    "        \n",
    "        encoder_mean = Dense(self.latent_dim, name='encoder_mean')(encoder_flatten)\n",
    "        encoder_var = Dense(self.latent_dim, name='encoder_variance')(encoder_flatten)\n",
    "        \n",
    "        self.encoder = Model(encoder_input, [encoder_mean, encoder_var], name='encoder')\n",
    "                \n",
    "        decoder_input = Input(shape=(self.latent_dim), name='decoder_input')\n",
    "        decoder_dense = Dense(units=256, activation='relu', name='decoder_dense')(decoder_input)\n",
    "        x = Reshape(target_shape=(1, 1, 256), name='decoder_reshape')(decoder_dense)\n",
    "        x = Conv2DTranspose(filters=128, kernel_size=(8, 2), strides=(8, 2), padding='valid', activation='relu', name='decoder_conv2dtranspose_1')(x)\n",
    "        x = Conv2DTranspose(filters=64, kernel_size=(4, 4), strides=(4, 4), padding='valid', activation='relu', name='decoder_conv2dtranspose_2')(x)\n",
    "        x = Conv2DTranspose(filters=1, kernel_size=(4, 4), strides=(4, 4), padding='valid', name='decoder_conv2dtranspose_3')(x)\n",
    "        \n",
    "        decoder_output = x\n",
    "        #decoder_output = Reshape(target_shape=(128, self.sequence_length), name='decoder_output')(x)\n",
    "        \n",
    "        self.decoder = Model(decoder_input, decoder_output, name='decoder')\n",
    "        \n",
    "    def compile(self, optimizer):\n",
    "        super(CVAE, self).compile()\n",
    "        \n",
    "        self.optimizer = optimizer\n",
    "        self.encoder.compile(optimizer=self.optimizer)\n",
    "        self.decoder.compile(optimizer=self.optimizer)\n",
    "        \n",
    "    def call(self, inputs, is_training=False):\n",
    "        \n",
    "        inputs_is_list = isinstance(inputs, list)\n",
    "        \n",
    "        if inputs_is_list and is_training:\n",
    "            return [self.train_step(x_y) for x_y in inputs]\n",
    "        \n",
    "        elif inputs_is_list and not is_training:\n",
    "            return [self.test_step(x_y) for x_y in inputs]\n",
    "        \n",
    "        elif not inputs_is_list and is_training:\n",
    "            return self.train_step(x_y)\n",
    "        \n",
    "        elif not inputs_is_list and not is_training:\n",
    "            return self.test_step(x_y)\n",
    "        \n",
    "    def log_normal_pdf(self, sample, mean, logvar, raxis=1):\n",
    "          \n",
    "        ln2pi = tf.math.log(2. * np.pi)\n",
    "        x = -.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + ln2pi)\n",
    "        return tf.reduce_sum(x, axis=raxis)\n",
    "        \n",
    "    def compute_loss(self, x):\n",
    "                \n",
    "        mean, logvar = self.encoder(x)\n",
    "        z = self.reparameterize(mean, logvar)\n",
    "        x_logit = self.decode(z)\n",
    "                \n",
    "        # tf crossentropy\n",
    "        cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            logits=x_logit, labels=x\n",
    "        )\n",
    "        \n",
    "        # K crossentropy\n",
    "        #cross_entropy = K.binary_crossentropy(target=x, output=x_logit)    \n",
    "        \n",
    "        logpx_z = -tf.reduce_sum(cross_ent, axis=[1, 2, 3])\n",
    "        logpz = self.log_normal_pdf(z, 0., 0.)\n",
    "        logqz_x = self.log_normal_pdf(z, mean, logvar)\n",
    "        return -tf.reduce_mean(logpx_z + logpz - logqz_x)    \n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, x):\n",
    "        \n",
    "        # print(tf.executing_eagerly()) --> False\n",
    "                \n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self.compute_loss(x)\n",
    "        \n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        return {'loss': loss}\n",
    "    \n",
    "    def get_latent_vect(self, x):\n",
    "        \n",
    "        mean, logvar = self.encoder(x)\n",
    "        return self.reparameterize(mean, logvar)\n",
    "    \n",
    "    def test_step(self, x):\n",
    "        \n",
    "        loss = self.compute_loss(x)\n",
    "        return {'loss': loss}\n",
    "    \n",
    "    def reparameterize(self, mean, logvar):\n",
    "        eps = tf.random.normal(shape=mean.shape)\n",
    "        return eps * tf.exp(logvar * .5) + mean\n",
    "        \n",
    "    def decode(self, z, apply_sigmoid=False):\n",
    "        logits = self.decoder(z)\n",
    "        if apply_sigmoid:\n",
    "            return tf.sigmoid(logits)\n",
    "        return logits\n",
    "        \n",
    "    @tf.function\n",
    "    def sample(self, eps=None):\n",
    "        if eps is None:\n",
    "            eps = tf.random.normal(shape=(100, self.latent_dim))\n",
    "        return self.decode(eps, apply_sigmoid=True)\n",
    "    \n",
    "    def generate(self, x):\n",
    "        \n",
    "        if isinstance(x, tf.sparse.SparseTensor):\n",
    "            x = tf.sparse.to_dense(x)\n",
    "            \n",
    "        mean, logvar = self.encoder(x)\n",
    "        z = self.reparameterize(mean, logvar)\n",
    "        return self.sample(z)            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ba669d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvae = CVAE(LATENT_DIM, SEQUENCE_LENGTH)\n",
    "cvae.compile(tf.keras.optimizers.Adam(1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5b87732d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvae.encoder.load_weights('C:/_local/py/yt_piano_music_gen/models/cvae/encoder_64_z_64_seq_15_epochs_288_loss.hdf5')\n",
    "cvae.decoder.load_weights('C:/_local/py/yt_piano_music_gen/models/cvae/decoder_64_z_64_seq_15_epochs_288_loss.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bea75dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_input (InputLayer)     [(None, 128, 50, 1)  0           []                               \n",
      "                                ]                                                                 \n",
      "                                                                                                  \n",
      " encoder_conv2d_1 (Conv2D)      (None, 32, 13, 64)   1088        ['encoder_input[0][0]']          \n",
      "                                                                                                  \n",
      " encoder_conv2d_2 (Conv2D)      (None, 8, 4, 128)    131200      ['encoder_conv2d_1[0][0]']       \n",
      "                                                                                                  \n",
      " encoder_conv2d_3 (Conv2D)      (None, 1, 2, 256)    524544      ['encoder_conv2d_2[0][0]']       \n",
      "                                                                                                  \n",
      " encoder_flatten (Flatten)      (None, 512)          0           ['encoder_conv2d_3[0][0]']       \n",
      "                                                                                                  \n",
      " encoder_mean (Dense)           (None, 16)           8208        ['encoder_flatten[0][0]']        \n",
      "                                                                                                  \n",
      " encoder_variance (Dense)       (None, 16)           8208        ['encoder_flatten[0][0]']        \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 673,248\n",
      "Trainable params: 673,248\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cvae.encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6186406d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 16)]              0         \n",
      "                                                                 \n",
      " decoder_dense (Dense)       (None, 256)               4352      \n",
      "                                                                 \n",
      " decoder_reshape (Reshape)   (None, 1, 2, 128)         0         \n",
      "                                                                 \n",
      " decoder_conv2dtranspose_1 (  (None, 8, 4, 128)        262272    \n",
      " Conv2DTranspose)                                                \n",
      "                                                                 \n",
      " decoder_conv2dtranspose_2 (  (None, 32, 16, 64)       131136    \n",
      " Conv2DTranspose)                                                \n",
      "                                                                 \n",
      " decoder_conv2dtranspose_3 (  (None, 128, 64, 1)       1025      \n",
      " Conv2DTranspose)                                                \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 398,785\n",
      "Trainable params: 398,785\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cvae.decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "886f75f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAECheckpointCallback(tf.keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, model_dir):\n",
    "        \n",
    "        self.model_dir = model_dir\n",
    "        \n",
    "        self.best_loss = np.Inf\n",
    "        self.best_epoch = 0\n",
    "        \n",
    "        self.best_encoder = None\n",
    "        self.best_decoder = None\n",
    "        self.best_model = None\n",
    "                        \n",
    "    def on_epoch_end(self, epoch, loss_dict):\n",
    "                        \n",
    "        if loss_dict['loss'] < self.best_loss:\n",
    "            \n",
    "            self.best_epoch = epoch\n",
    "            self.best_loss = loss_dict['loss']   \n",
    "            \n",
    "            self.best_encoder = self.model.encoder\n",
    "            self.best_decoder = self.model.decoder\n",
    "            self.best_model = self.model\n",
    "    \n",
    "    def on_train_end(self, loss_dict):\n",
    "        \n",
    "        encoder_file_name = f'encoder_{self.best_epoch}_epochs_{round(self.best_loss)}_loss'\n",
    "        decoder_file_name = f'decoder_{self.best_epoch}_epochs_{round(self.best_loss)}_loss'\n",
    "        cvae_file_name = f'cvae_{self.best_epoch}_epochs_{round(self.best_loss)}_loss'\n",
    "        \n",
    "        encoder_save_path = self.model_dir + encoder_file_name\n",
    "        decoder_save_path = self.model_dir + decoder_file_name\n",
    "        cvae_save_path = self.model_dir + cvae_file_name\n",
    "        \n",
    "        self.best_encoder.save(encoder_save_path)\n",
    "        self.best_decoder.save(decoder_save_path)\n",
    "        self.model.save(cvae_save_path)\n",
    "    \n",
    "cvae_ckpt_clbk = CVAECheckpointCallback(CVAE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "fdfd885e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvae_reduce_lr_clbk = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=.1, min_delta=5, patience=3)\n",
    "cvae_early_stop_clbk = tf.keras.callbacks.EarlyStopping(monitor='loss', min_delta=5, patience=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ccf0c5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 110s 110ms/step - loss: 183.9410 - val_loss: 154.4644 - lr: 1.0000e-04\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 102s 102ms/step - loss: 183.1399 - val_loss: 203.1646 - lr: 1.0000e-04\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 146s 146ms/step - loss: 182.9885 - val_loss: 174.0713 - lr: 1.0000e-04\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 182s 182ms/step - loss: 183.7847 - val_loss: 179.0745 - lr: 1.0000e-04\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 138s 138ms/step - loss: 182.3225 - val_loss: 151.3802 - lr: 1.0000e-05\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 138s 138ms/step - loss: 182.1129 - val_loss: 196.5403 - lr: 1.0000e-05\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 195s 195ms/step - loss: 180.8857 - val_loss: 177.1658 - lr: 1.0000e-05\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 185s 185ms/step - loss: 182.9768 - val_loss: 177.1575 - lr: 1.0000e-05\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 158s 158ms/step - loss: 180.8371 - val_loss: 157.5794 - lr: 1.0000e-05\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 157s 157ms/step - loss: 182.6572 - val_loss: 171.3037 - lr: 1.0000e-06\n",
      "INFO:tensorflow:Assets written to: C:/_local/py/yt_piano_music_gen/models/cvae/5_epochs_151_loss\\assets\n",
      "INFO:tensorflow:Assets written to: C:/_local/py/yt_piano_music_gen/models/cvae/5_epochs_151_loss\\assets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1359d7bf820>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvae.fit(x=cvae_train_dataset, batch_size=BATCH_SIZE,\n",
    "         epochs=10, steps_per_epoch=1000,\n",
    "         validation_data=cvae_test_dataset, validation_steps=150,\n",
    "         shuffle=False,\n",
    "         callbacks=[cvae_ckpt_clbk, \n",
    "                    cvae_reduce_lr_clbk, \n",
    "                    cvae_early_stop_clbk]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ef267e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample # 29\n"
     ]
    }
   ],
   "source": [
    "play_samples_from_batch(sample_input.numpy(), 15, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c5ceb7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_output = cvae.generate(sample_input).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7159ef31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample # 30\n"
     ]
    }
   ],
   "source": [
    "play_samples_from_batch(sample_output, 15, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55f6a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BEST_ENCODER_PATH = 'C:/_local/py/yt_piano_music_gen/models/cvae/encoder_5_epochs_466_elbo.hdf5'\n",
    "#BEST_DECODER_PATH = 'C:/_local/py/yt_piano_music_gen/models/cvae/decoder_5_epochs_466_elbo.hdf5'\n",
    "\n",
    "#BEST_ENCODER_PATH = 'C:/_local/py/yt_piano_music_gen/models/cvae/seq_32_encoder_15_epochs_275_elbo.hdf5'\n",
    "#BEST_DECODER_PATH = 'C:/_local/py/yt_piano_music_gen/models/cvae/seq_32_decoder_15_epochs_275_elbo.hdf5'\n",
    "\n",
    "#BEST_ENCODER_PATH = 'C:/_local/py/yt_piano_music_gen/models/cvae/seq_64_encoder_2_epochs_317_elbo.hdf5'\n",
    "#BEST_DECODER_PATH = 'C:/_local/py/yt_piano_music_gen/models/cvae/seq_32_decoder_2_epochs_317_elbo.hdf5'\n",
    "\n",
    "cvae = CVAE(CVAE_LATENT_DIM, SEQUENCE_LENGTH)\n",
    "cvae.compile(tf.keras.optimizers.Adam(1e-3))\n",
    "\n",
    "cvae.load_encoder(BEST_ENCODER_PATH)\n",
    "cvae.load_decoder(BEST_DECODER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "51754723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115.31005674600601"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvae(sample_input).numpy().max()*127."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8d72ebec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample # 14\n"
     ]
    }
   ],
   "source": [
    "# Play some randomly generated samples\n",
    "# np.random.seed(5)\n",
    "sample_number = 15\n",
    "sample_output = cvae.sample().numpy()*127.\n",
    "\n",
    "play_samples(sample_output, sample_number)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723d9909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Playing samples reconstructed from sample inputs\n",
    "np.random.seed(6)\n",
    "sample_output = cvae.generate(sample_input).numpy()*127.\n",
    "\n",
    "play_samples(sample_output, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9761e4c6",
   "metadata": {},
   "source": [
    "## CVAE Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "cefe9e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(1e-3)\n",
    "\n",
    "X = Input(shape=(128, SEQUENCE_LENGTH), name='input')\n",
    "X_add_channel = Reshape(target_shape=(128, SEQUENCE_LENGTH, 1), name='encoder_input_reshape')(X)\n",
    "\n",
    "encoder_conv_1 = Conv2D(filters=64, kernel_size=(4, 4), strides=(4, 4), \n",
    "                        activation='relu', padding='same', name='conv2d_1')(X_add_channel)\n",
    "\n",
    "#encoder_relu_1 = Relu(name='encoder_relu_1')(encoder_batch_norm_1)\n",
    "#encoder_batch_norm_1 = BatchNorm(name='encoder_batch_norm_1')(encoder_conv_1)\n",
    "\n",
    "encoder_conv_2 = Conv2D(filters=128, kernel_size=(4, 4), strides=(4, 4), \n",
    "                        activation='relu', padding='same', name='conv2d_2')(encoder_conv_1)\n",
    "\n",
    "#encoder_batch_norm_2 = BatchNorm(name='encoder_batch_norm_2')(encoder_conv_2)\n",
    "#encoder_relu_2 = Relu(name='encoder_relu_2')(encoder_batch_norm_2)\n",
    "\n",
    "encoder_conv_3 = Conv2D(filters=256, kernel_size=(8, 2), strides=(8, 2), \n",
    "                        activation='relu', padding='same', name='conv2d_3')(encoder_conv_2)\n",
    "\n",
    "#encoder_batch_norm_3 = BatchNorm(name='encoder_batch_norm_3')(encoder_conv_3)\n",
    "#encoder_relu_3 = Relu(name='encoder_relu_3')(encoder_batch_norm_3)\n",
    "\n",
    "encoder_flatten = Flatten(name='encoder_flatten')(encoder_conv_3)\n",
    "\n",
    "encoder_dense_1 = Dense(256, activation='relu', name='encoder_dense_1')(encoder_flatten)\n",
    "encoder_dense_2 = Dense(256, activation='relu', name='encoder_dense_2')(encoder_dense_1)\n",
    "encoder_dense_3 = Dense(256, activation='relu', name='encoder_dense_3')(encoder_dense_2)\n",
    "\n",
    "encoder_mu = Dense(LATENT_DIM, activation='linear', name='encoder_mu')(encoder_dense_3)\n",
    "encoder_log_sigma = Dense(LATENT_DIM, activation='linear', name='encoder_log_sigma')(encoder_dense_3)\n",
    "\n",
    "encoder = Model(X, encoder_mu, name='encoder')\n",
    "encoder.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "b60b2317",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 128, 32)]         0         \n",
      "                                                                 \n",
      " encoder_input_reshape (Resh  (None, 128, 32, 1)       0         \n",
      " ape)                                                            \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 32, 8, 64)         1088      \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 8, 2, 128)         131200    \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 1, 1, 256)         524544    \n",
      "                                                                 \n",
      " encoder_flatten (Flatten)   (None, 256)               0         \n",
      "                                                                 \n",
      " encoder_dense_1 (Dense)     (None, 256)               65792     \n",
      "                                                                 \n",
      " encoder_dense_2 (Dense)     (None, 256)               65792     \n",
      "                                                                 \n",
      " encoder_dense_3 (Dense)     (None, 256)               65792     \n",
      "                                                                 \n",
      " encoder_mu (Dense)          (None, 128)               32896     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 887,104\n",
      "Trainable params: 887,104\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "b4ba15de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reparameterization and concatenation layers / specification\n",
    "def sample_and_reparameterize(mu_logsigma):\n",
    "    mu, log_sigma = mu_logsigma\n",
    "    \n",
    "    b, k = mu.shape\n",
    "    \n",
    "    eps = K.random_normal(shape=(k,))\n",
    "    return mu + K.exp(log_sigma * .5) * eps\n",
    "\n",
    "Z = Lambda(sample_and_reparameterize, output_shape=(LATENT_DIM,),\n",
    "           name='sample_and_reparameterize')([encoder_mu, encoder_log_sigma])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "c157861c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard decoder declaration\n",
    "\n",
    "decoder_dense_1_layer = Dense(256, activation='relu', name='decoder_dense_1')\n",
    "decoder_dense_2_layer = Dense(256, activation='relu', name='decoder_dense_2')\n",
    "decoder_dense_3_layer = Dense(256, activation='relu', name='decoder_dense_3')\n",
    "decoder_reshape_1_layer = Reshape(target_shape=(1, 1, 256), name='reshape')\n",
    "\n",
    "decoder_conv_1_layer = Conv2DTranspose(filters=128, kernel_size=(8, 2), strides=(8, 2), \n",
    "                                       activation='relu', padding='valid', \n",
    "                                       name='conv2dtranspose_1')\n",
    "\n",
    "decoder_conv_2_layer = Conv2DTranspose(filters=64, kernel_size=(4, 4), strides=(4, 4), \n",
    "                                       activation='relu', padding='valid', \n",
    "                                       name='conv2dtranspose_2')\n",
    "\n",
    "decoder_conv_3_layer = Conv2DTranspose(filters=1, kernel_size=(4, 4), strides=(4, 4), \n",
    "                                       activation='linear', padding='valid', name='conv2dtranspose_3')\n",
    "\n",
    "decoder_output_layer = Reshape(target_shape=(128, SEQUENCE_LENGTH), name='decoder_output')\n",
    "\n",
    "d1 = decoder_dense_1_layer(Z)\n",
    "d2 = decoder_dense_2_layer(d1)\n",
    "d3 = decoder_dense_3_layer(d2)\n",
    "d4 = decoder_reshape_1_layer(d3)\n",
    "d5 = decoder_conv_1_layer(d4)\n",
    "d6 = decoder_conv_2_layer(d5)\n",
    "d7 = decoder_conv_3_layer(d6)\n",
    "cvae_output = decoder_output_layer(d7)\n",
    "\n",
    "decoder_input = Input(shape=(LATENT_DIM,), name='decoder_input')\n",
    "decoder_dense_1 = decoder_dense_1_layer(decoder_input)\n",
    "decoder_dense_2 = decoder_dense_2_layer(decoder_dense_1)\n",
    "decoder_dense_3 = decoder_dense_3_layer(decoder_dense_2)\n",
    "decoder_reshape_1 = decoder_reshape_1_layer(decoder_dense_3)\n",
    "decoder_conv_1 = decoder_conv_1_layer(decoder_reshape_1)\n",
    "decoder_conv_2 = decoder_conv_2_layer(decoder_conv_1)\n",
    "decoder_conv_3 = decoder_conv_3_layer(decoder_conv_2)\n",
    "decoder_output = decoder_output_layer(decoder_conv_3)\n",
    "\n",
    "decoder = Model(decoder_input, decoder_output, name='decoder')\n",
    "decoder.compile(optimizer=optimizer)\n",
    "\n",
    "cvae = Model(X, cvae_output, name='cvae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "3a986b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 128)]             0         \n",
      "                                                                 \n",
      " decoder_dense_1 (Dense)     (None, 256)               33024     \n",
      "                                                                 \n",
      " decoder_dense_2 (Dense)     (None, 256)               65792     \n",
      "                                                                 \n",
      " decoder_dense_3 (Dense)     (None, 256)               65792     \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 1, 1, 256)         0         \n",
      "                                                                 \n",
      " conv2dtranspose_1 (Conv2DTr  (None, 8, 2, 128)        524416    \n",
      " anspose)                                                        \n",
      "                                                                 \n",
      " conv2dtranspose_2 (Conv2DTr  (None, 32, 8, 64)        131136    \n",
      " anspose)                                                        \n",
      "                                                                 \n",
      " conv2dtranspose_3 (Conv2DTr  (None, 128, 32, 1)       1025      \n",
      " anspose)                                                        \n",
      "                                                                 \n",
      " decoder_output (Reshape)    (None, 128, 32)           0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 821,185\n",
      "Trainable params: 821,185\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "ffb48286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"cvae\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input (InputLayer)             [(None, 128, 32)]    0           []                               \n",
      "                                                                                                  \n",
      " encoder_input_reshape (Reshape  (None, 128, 32, 1)  0           ['input[0][0]']                  \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 32, 8, 64)    1088        ['encoder_input_reshape[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 8, 2, 128)    131200      ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 1, 1, 256)    524544      ['conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " encoder_flatten (Flatten)      (None, 256)          0           ['conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " encoder_dense_1 (Dense)        (None, 256)          65792       ['encoder_flatten[0][0]']        \n",
      "                                                                                                  \n",
      " encoder_dense_2 (Dense)        (None, 256)          65792       ['encoder_dense_1[0][0]']        \n",
      "                                                                                                  \n",
      " encoder_dense_3 (Dense)        (None, 256)          65792       ['encoder_dense_2[0][0]']        \n",
      "                                                                                                  \n",
      " encoder_mu (Dense)             (None, 128)          32896       ['encoder_dense_3[0][0]']        \n",
      "                                                                                                  \n",
      " encoder_log_sigma (Dense)      (None, 128)          32896       ['encoder_dense_3[0][0]']        \n",
      "                                                                                                  \n",
      " sample_and_reparameterize (Lam  (None, 128)         0           ['encoder_mu[0][0]',             \n",
      " bda)                                                             'encoder_log_sigma[0][0]']      \n",
      "                                                                                                  \n",
      " decoder_dense_1 (Dense)        (None, 256)          33024       ['sample_and_reparameterize[0][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " decoder_dense_2 (Dense)        (None, 256)          65792       ['decoder_dense_1[0][0]']        \n",
      "                                                                                                  \n",
      " decoder_dense_3 (Dense)        (None, 256)          65792       ['decoder_dense_2[0][0]']        \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 1, 1, 256)    0           ['decoder_dense_3[0][0]']        \n",
      "                                                                                                  \n",
      " conv2dtranspose_1 (Conv2DTrans  (None, 8, 2, 128)   524416      ['reshape[0][0]']                \n",
      " pose)                                                                                            \n",
      "                                                                                                  \n",
      " conv2dtranspose_2 (Conv2DTrans  (None, 32, 8, 64)   131136      ['conv2dtranspose_1[0][0]']      \n",
      " pose)                                                                                            \n",
      "                                                                                                  \n",
      " conv2dtranspose_3 (Conv2DTrans  (None, 128, 32, 1)  1025        ['conv2dtranspose_2[0][0]']      \n",
      " pose)                                                                                            \n",
      "                                                                                                  \n",
      " decoder_output (Reshape)       (None, 128, 32)      0           ['conv2dtranspose_3[0][0]']      \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,741,185\n",
      "Trainable params: 1,741,185\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cvae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "49cb5f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras Loss functions\n",
    "\n",
    "def reconstruction_loss(x_true, x_pred):\n",
    "    \n",
    "    cross_entropy = K.binary_crossentropy(target=x_true, output=x_pred)    \n",
    "    return K.sum(cross_entropy, axis=[1, 2])\n",
    "\n",
    "def kl_divergence(emu, els):\n",
    "    \n",
    "    kl_2 = K.exp(els) + K.square(emu) - 1. - els\n",
    "    kl = .5 * K.sum(kl_2, axis=-1)\n",
    "    return kl #*10\n",
    "\n",
    "# Tensorflow loss functions\n",
    "\n",
    "#def reconstruction_loss(x_true, x_pred):\n",
    "    \n",
    "#    cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=x_true, logits=x_pred)    \n",
    "#    return tf.reduce_sum(cross_entropy, axis=-1)\n",
    "\n",
    "#def kl_divergence(emu, els):\n",
    "    \n",
    "#    kl_2 = tf.exp(els) + tf.square(emu) - 1. - els\n",
    "#    return .5 * tf.reduce_sum(kl_2, axis=-1)\n",
    "\n",
    "\n",
    "def vae_loss(x_true, x_pred, emu, els):\n",
    "    \n",
    "    recon_loss = reconstruction_loss(x_true, x_pred)\n",
    "    kl_loss = kl_divergence(emu, els)\n",
    "    \n",
    "    return recon_loss + kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "1a2bb937",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(cvae.losses) == 0:\n",
    "    cvae.add_loss(vae_loss(X, cvae_output, encoder_mu, encoder_log_sigma))\n",
    "    \n",
    "if len(cvae.metrics) == 0:\n",
    "    cvae.add_metric(reconstruction_loss(X, cvae_output), name='reconstruction_loss')\n",
    "    cvae.add_metric(kl_divergence(encoder_mu, encoder_log_sigma), name='kl_divergence')\n",
    "    \n",
    "cvae.compile(optimizer=tf.keras.optimizers.Adam(1e-3), loss=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "5bb69c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAECheckpointCallback(tf.keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, model_dir):\n",
    "        \n",
    "        self.encoder_dir = model_dir + 'encoder/'\n",
    "        self.decoder_dir = model_dir + 'decoder/'\n",
    "        self.cvae_dir = model_dir + 'cvae/'\n",
    "        \n",
    "        self.best_encoder = None\n",
    "        self.best_decoder = None\n",
    "        self.best_cvae = None\n",
    "        \n",
    "        self.best_epoch = 0\n",
    "        self.best_loss = np.Inf\n",
    "        \n",
    "    def on_epoch_end(self, epoch, loss_dict):\n",
    "                        \n",
    "        if loss_dict['loss'] < self.best_loss:\n",
    "            \n",
    "            self.best_epoch = epoch\n",
    "            self.best_loss = loss_dict['loss']   \n",
    "            \n",
    "            global encoder\n",
    "            global decoder\n",
    "            global cvae\n",
    "            global SEQUENCE_LENGTH\n",
    "            global LATENT_DIM\n",
    "            \n",
    "            self.best_encoder = encoder\n",
    "            self.best_decoder = decoder\n",
    "            self.best_cvae = self.model\n",
    "    \n",
    "    def on_train_end(self, loss_dict):\n",
    "        \n",
    "        encoder_file_name = f'encoder_s_{SEQUENCE_LENGTH}_z_{LATENT_DIM}_{self.best_epoch}_epochs_{round(self.best_loss, 2)}_loss'\n",
    "        decoder_file_name = f'decoder_s_{SEQUENCE_LENGTH}_z_{LATENT_DIM}_{self.best_epoch}_epochs_{round(self.best_loss, 2)}_loss'\n",
    "        cvae_file_name = f'cvae_s_{SEQUENCE_LENGTH}_z_{LATENT_DIM}_{self.best_epoch}_epochs_{round(self.best_loss)}_loss'\n",
    "        \n",
    "        encoder_save_path = self.encoder_dir + encoder_file_name\n",
    "        decoder_save_path = self.decoder_dir + decoder_file_name\n",
    "        cvae_save_path = self.cvae_dir + cvae_file_name\n",
    "        \n",
    "        self.best_encoder.save(encoder_save_path)\n",
    "        self.best_decoder.save(decoder_save_path)\n",
    "        self.best_cvae.save(cvae_save_path)\n",
    "\n",
    "vae_ckpt_clbk = VAECheckpointCallback(NEW_VAE_DIR)\n",
    "vae_reduce_lr_clbk = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', min_delta=3, patience=3, factor=.1)\n",
    "vae_early_stop_clbk = tf.keras.callbacks.EarlyStopping(monitor='loss', min_delta=3, patience=6)\n",
    "\n",
    "vae_clbks = [vae_ckpt_clbk, \n",
    "             vae_reduce_lr_clbk, \n",
    "             vae_early_stop_clbk\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "fff50fd6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2048/2048 [==============================] - 188s 92ms/step - loss: 327.5081 - reconstruction_loss: 320.5455 - kl_divergence: 6.9632 - val_loss: 312.0131 - val_reconstruction_loss: 305.4472 - val_kl_divergence: 6.5659 - lr: 0.0010\n",
      "Epoch 2/5\n",
      "2048/2048 [==============================] - 301s 147ms/step - loss: 323.6011 - reconstruction_loss: 315.0773 - kl_divergence: 8.5235 - val_loss: 303.9997 - val_reconstruction_loss: 294.5423 - val_kl_divergence: 9.4574 - lr: 0.0010\n",
      "Epoch 3/5\n",
      "2048/2048 [==============================] - 323s 158ms/step - loss: 314.4076 - reconstruction_loss: 304.3759 - kl_divergence: 10.0317 - val_loss: 295.8372 - val_reconstruction_loss: 286.1662 - val_kl_divergence: 9.6710 - lr: 0.0010\n",
      "Epoch 4/5\n",
      "2048/2048 [==============================] - 345s 169ms/step - loss: 305.3665 - reconstruction_loss: 294.7558 - kl_divergence: 10.6108 - val_loss: 290.0060 - val_reconstruction_loss: 279.9221 - val_kl_divergence: 10.0839 - lr: 0.0010\n",
      "Epoch 5/5\n",
      "2048/2048 [==============================] - 345s 168ms/step - loss: 299.3535 - reconstruction_loss: 288.8916 - kl_divergence: 10.4623 - val_loss: 290.4229 - val_reconstruction_loss: 279.4238 - val_kl_divergence: 10.9991 - lr: 0.0010\n",
      "INFO:tensorflow:Assets written to: C:/_local/py/yt_piano_music_gen/models/new_cvae/encoder/encoder_s_32_z_128_4_epochs_299.35_loss\\assets\n",
      "INFO:tensorflow:Assets written to: C:/_local/py/yt_piano_music_gen/models/new_cvae/decoder/decoder_s_32_z_128_4_epochs_299.35_loss\\assets\n",
      "INFO:tensorflow:Assets written to: C:/_local/py/yt_piano_music_gen/models/new_cvae/cvae/cvae_s_32_z_128_4_epochs_299_loss\\assets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e723d430a0>"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvae.fit(x=cvae_train_dataset, shuffle=False,\n",
    "         batch_size=BATCH_SIZE, steps_per_epoch=2048, \n",
    "         epochs=5,\n",
    "         validation_data=cvae_test_dataset, validation_steps=256,\n",
    "         callbacks=vae_clbks\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "af25f90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:/_local/py/yt_piano_music_gen/models/new_cvae/encoder/encoder_s_32_z_128_epochs_30_loss_242\\assets\n",
      "INFO:tensorflow:Assets written to: C:/_local/py/yt_piano_music_gen/models/new_cvae/decoder/decoder_seq_32_z_128_epochs_30_loss_242\\assets\n"
     ]
    }
   ],
   "source": [
    "min_val_loss = round(min(cvae.history.history['val_loss']))\n",
    "\n",
    "ENCODER_DIR = 'C:/_local/py/yt_piano_music_gen/models/new_cvae/encoder/'\n",
    "ENCODER_FILE_NAME = f'encoder_s_{SEQUENCE_LENGTH}_z_{LATENT_DIM}_epochs_30_loss_{min_val_loss}'\n",
    "ENCODER_PATH = ENCODER_DIR + ENCODER_FILE_NAME\n",
    "\n",
    "DECODER_DIR = 'C:/_local/py/yt_piano_music_gen/models/new_cvae/decoder/'\n",
    "DECODER_FILE_NAME = f'decoder_seq_{SEQUENCE_LENGTH}_z_{LATENT_DIM}_epochs_30_loss_{min_val_loss}'\n",
    "DECODER_PATH = DECODER_DIR + DECODER_FILE_NAME\n",
    "\n",
    "encoder.save(ENCODER_PATH)\n",
    "decoder.save(DECODER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "83c77b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(decoder, num_to_generate):\n",
    "    \n",
    "    eps = tf.random.normal(shape=(num_to_generate, LATENT_DIM))\n",
    "    logits = decoder(eps)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "705a5862",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_output = tf.reshape(generate(decoder, 100), shape=(100, 128, SEQUENCE_LENGTH))\n",
    "sample_output = sample_output.numpy() * 127."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e922fd53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92.949104"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_output.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0d4fe48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample # 97\n"
     ]
    }
   ],
   "source": [
    "play_samples_from_batch(sample_output, 25, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c6fdcc",
   "metadata": {},
   "source": [
    "### CVAE VERSION 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "b38f5f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyVAE(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, latent_dim, sequence_length):\n",
    "        super(MyVAE, self).__init__()\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "        encoder_input = Input(shape=(128, self.sequence_length), name='encoder_input')\n",
    "        encoder_reshape = Reshape(target_shape=(128, self.sequence_length, 1), name='encoder_reshape')(encoder_input)\n",
    "        encoder_conv_1 = Conv2D(filters=64,kernel_size=(4, 4), strides=(4, 4), activation='relu',name='encoder_conv2d_1')(encoder_reshape)\n",
    "        encoder_conv_2 = Conv2D(filters=128, kernel_size=(4, 4), strides=(4, 4), activation='relu', name='encoder_conv2d_2')(encoder_conv_1)\n",
    "        encoder_conv_3 = Conv2D(filters=256, kernel_size=(8, 2), strides=(8, 2), activation='relu', name='encoder_conv2d_3')(encoder_conv_2)\n",
    "        encoder_flatten = Flatten(name='encoder_flatten')(encoder_conv_3)\n",
    "        \n",
    "        encoder_output = Dense(self.latent_dim+1, name='decoder_output')(encoder_flatten)\n",
    "        \n",
    "        self.encoder = Model(encoder_input, encoder_output, name='encoder')\n",
    "        \n",
    "        Z = Lambda(self.sample_and_reparameterize, output_shape=(self.latent_dim), arguments={'n_samples': 1})(encoder_output)\n",
    "        \n",
    "        #decoder_input = Input(shape=(self.latent_dim), name='decoder_input')\n",
    "        decoder_dense = Dense(units=256, activation='relu', name='decoder_dense')(Z)\n",
    "        x = Reshape(target_shape=(1, 1, 256), name='decoder_reshape')(decoder_dense)\n",
    "        x = Conv2DTranspose(filters=128, kernel_size=(8, 4), strides=(8, 4), padding='same', activation='relu', name='decoder_conv2dtranspose_1')(x)\n",
    "        x = Conv2DTranspose(filters=64, kernel_size=(4, 4), strides=(4, 4), padding='same', activation='relu', name='decoder_conv2dtranspose_2')(x)\n",
    "        x = Conv2DTranspose(filters=1, kernel_size=(4, 4), strides=(4, 4), padding='same', activation='linear', name='decoder_conv2dtranspose_3')(x)\n",
    "        decoder_output = Reshape(target_shape=(128, self.sequence_length), name='decoder_output')(x)\n",
    "        \n",
    "        self.decoder = Model(decoder_input, decoder_output, name='decoder')\n",
    "     \n",
    "    def sample_and_reparameterize(self, mu_log_sigma, n_samples):\n",
    "        \n",
    "        b, k = mu_log_sigma.shape\n",
    "        k -= 1\n",
    "        \n",
    "        mu, sigma = mu_log_sigma[:, :-1], tf.exp(mu_log_sigma[:, -1])\n",
    "        eps = tf.random.normal(shape=(b, n_samples, k))\n",
    "        return eps * tf.reshape(sigma, (b, 1, 1)) + tf.reshape(mu, (b, 1, k))\n",
    "    \n",
    "    def log_p_x(self, x, mu_x, sigma_x):\n",
    "        \n",
    "        b, n = x.shape[:2]\n",
    "        \n",
    "        x = tf.reshape(x, (b, 1, -1))\n",
    "        _, _, p = x.shape\n",
    "        \n",
    "        square_error = (x - tf.reshape(mu_x, (b, n, -1)))**2 / (2*sigma_x**2)\n",
    "        \n",
    "        log_sigma = tf.log(sigma_x)\n",
    "        log_sigma = tf.reduce_sum(log_sigma, axis=2)\n",
    "        log_sigma = tf.reduce_mean(log_sigma, axis=[0, 1])\n",
    "        \n",
    "        return -square_error - log_sigma\n",
    "        \n",
    "    def kl_q_p(self, z, mu_log_sigma):\n",
    "        \n",
    "        b, n, k = z.shape\n",
    "        mu_q, log_sigma_q = mu_log_sigma\n",
    "        \n",
    "        log_p = -.5 * tf.square(z)\n",
    "        \n",
    "        log_q = -.5 * tf.square(z - tf.reshape(mu_q, (b, 1, k)))\n",
    "        log_q /= tf.square(tf.reshape(tf.exp(log_sigma_q), (b, 1, 1)))\n",
    "        log_q -= tf.reshape(log_sigma_q, (b, 1, -1))\n",
    "        \n",
    "        kl = tf.reduce_sum(log_q - log_p, axis=2)\n",
    "        return tf.reduce_mean(kl, axis=[1, 1])\n",
    "        \n",
    "    def elbo(self, x, n=1):\n",
    "        \n",
    "        mu, log_sigma = self.encoder(x)\n",
    "        Z = self.sample_and_reparameterize(mu, log_sigma, n)\n",
    "        mu_x = self.decoder(Z)\n",
    "        \n",
    "        return log_p_x(x, mu_x, )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "9929d1f5",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Exception encountered when calling layer \"lambda_3\" (type Lambda).\n\nFailed to convert elements of (None, 1, 64) to Tensor. Consider casting elements to a supported type. See https://www.tensorflow.org/api_docs/python/tf/dtypes for supported TF dtypes.\n\nCall arguments received:\n   inputs=tf.Tensor(shape=(None, 65), dtype=float32)\n   mask=None\n   training=None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_69888/3410089951.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest_cvae\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMyVAE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLATENT_DIM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSEQUENCE_LENGTH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_69888/3517065687.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, latent_dim, sequence_length)\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoder_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'encoder'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mZ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLambda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_and_reparameterize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marguments\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'n_samples'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoder_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;31m#decoder_input = Input(shape=(self.latent_dim), name='decoder_input')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_69888/3517065687.py\u001b[0m in \u001b[0;36msample_and_reparameterize\u001b[1;34m(self, mu_log_sigma, n_samples)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msigma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmu_log_sigma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmu_log_sigma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[0meps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0meps\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msigma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Exception encountered when calling layer \"lambda_3\" (type Lambda).\n\nFailed to convert elements of (None, 1, 64) to Tensor. Consider casting elements to a supported type. See https://www.tensorflow.org/api_docs/python/tf/dtypes for supported TF dtypes.\n\nCall arguments received:\n   inputs=tf.Tensor(shape=(None, 65), dtype=float32)\n   mask=None\n   training=None"
     ]
    }
   ],
   "source": [
    "test_cvae = MyVAE(LATENT_DIM, SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d4236d",
   "metadata": {},
   "source": [
    "# Training CVAE on sequence of notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6546e757",
   "metadata": {},
   "source": [
    "## Creating TF Datasets of sequences of latent vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1765a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENCODER_DIR = 'C:/_local/py/yt_piano_music_gen/models/new_cvae/encoder/'\n",
    "DECODER_DIR = 'C:/_local/py/yt_piano_music_gen/models/new_cvae/decoder/'\n",
    "\n",
    "ENCODER_FILE_NAME = 'encoder_8_epochs_269_loss'\n",
    "DECODER_FILE_NAME = 'decoder_8_epochs_269_loss'\n",
    "\n",
    "#ENCODER_FILE_NAME = 'encoder_7_epochs_235_loss'\n",
    "#DECODER_FILE_NAME = 'decoder_7_epochs_235_loss'\n",
    "\n",
    "#ENCODER_FILE_NAME = f'encoder_s_{SEQUENCE_LENGTH}_z_{LATENT_DIM}_epochs_{epochs}_loss_242'\n",
    "#DECODER_FILE_NAME = f'decoder_seq_{SEQUENCE_LENGTH}_z_{LATENT_DIM}_epochs_{epochs}_loss_242'\n",
    "\n",
    "ENCODER_PATH = ENCODER_DIR + ENCODER_FILE_NAME\n",
    "DECODER_PATH = DECODER_DIR + DECODER_FILE_NAME\n",
    "\n",
    "encoder = tf.keras.models.load_model(ENCODER_PATH)\n",
    "decoder = tf.keras.models.load_model(DECODER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02dfd93f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 128, 32)]         0         \n",
      "                                                                 \n",
      " encoder_input_reshape (Resh  (None, 128, 32, 1)       0         \n",
      " ape)                                                            \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 32, 8, 64)         1088      \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 8, 2, 128)         131200    \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 1, 1, 256)         524544    \n",
      "                                                                 \n",
      " encoder_flatten (Flatten)   (None, 256)               0         \n",
      "                                                                 \n",
      " encoder_dense_1 (Dense)     (None, 256)               65792     \n",
      "                                                                 \n",
      " encoder_dense_2 (Dense)     (None, 256)               65792     \n",
      "                                                                 \n",
      " encoder_dense_3 (Dense)     (None, 256)               65792     \n",
      "                                                                 \n",
      " encoder_mu (Dense)          (None, 64)                16448     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 870,656\n",
      "Trainable params: 870,656\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cdae4eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 64)]              0         \n",
      "                                                                 \n",
      " decoder_dense_1 (Dense)     (None, 256)               16640     \n",
      "                                                                 \n",
      " decoder_dense_2 (Dense)     (None, 256)               65792     \n",
      "                                                                 \n",
      " decoder_dense_3 (Dense)     (None, 256)               65792     \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 1, 1, 256)         0         \n",
      "                                                                 \n",
      " conv2dtranspose_1 (Conv2DTr  (None, 8, 2, 128)        524416    \n",
      " anspose)                                                        \n",
      "                                                                 \n",
      " conv2dtranspose_2 (Conv2DTr  (None, 32, 8, 64)        131136    \n",
      " anspose)                                                        \n",
      "                                                                 \n",
      " conv2dtranspose_3 (Conv2DTr  (None, 128, 32, 1)       1025      \n",
      " anspose)                                                        \n",
      "                                                                 \n",
      " decoder_output (Reshape)    (None, 128, 32)           0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 804,801\n",
      "Trainable params: 804,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2cf5bf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MelodyTargetSequenceGenerator:\n",
    "    \n",
    "    def __init__(self, sparse_tensors, \n",
    "                 melody_sequence_length, melody_number_of_sequences, \n",
    "                 target_sequence_length, seed=None):\n",
    "        \n",
    "        self.sparse_tensors = sparse_tensors\n",
    "        self.num_tensors = len(self.sparse_tensors)\n",
    "        \n",
    "        self.melody_sequence_length = melody_sequence_length\n",
    "        self.melody_number_of_sequences = melody_number_of_sequences\n",
    "        self.melody_total_length = self.melody_sequence_length * self.melody_number_of_sequences\n",
    "        \n",
    "        self.target_sequence_length = target_sequence_length\n",
    "                \n",
    "        self.seed = seed\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "        \n",
    "        while True:\n",
    "            \n",
    "            sparse_tensor = self.sparse_tensors[np.random.randint(0, self.num_tensors)]\n",
    "                                    \n",
    "            last_start = (sparse_tensor.shape[1] - self.melody_total_length - self.target_sequence_length - 3)\n",
    "            melody_start = np.random.randint(0, last_start)\n",
    "            target_start = melody_start + self.melody_total_length\n",
    "            \n",
    "            melody = tf.sparse.slice(sparse_tensor,\n",
    "                                     start=[0, melody_start],\n",
    "                                     size=[128, self.melody_total_length]\n",
    "                                    )\n",
    "            \n",
    "            melody = tf.sparse.to_dense(melody)\n",
    "            \n",
    "            melody = tf.split(melody, axis=1, num_or_size_splits=self.melody_number_of_sequences)\n",
    "            melody = tf.stack(melody)\n",
    "            \n",
    "            target = tf.sparse.slice(sparse_tensor,\n",
    "                                     start=[0, target_start],\n",
    "                                     size=[128, self.target_sequence_length]\n",
    "                                    )\n",
    "            \n",
    "            target = tf.sparse.to_dense(target)\n",
    "            #target = tf.expand_dims(target, 0)\n",
    "                \n",
    "            #yield melody, target\n",
    "            yield tf.squeeze(melody), target\n",
    "            \n",
    "                    \n",
    "    def __call__(self):\n",
    "        return self.__iter__()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "adc3fc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SIZE_OF_GENERATORS = 50\n",
    "\n",
    "MELODY_NUMBER_OF_SEQUENCES = 1\n",
    "\n",
    "MELODY_TOTAL_LENGTH = MELODY_NUMBER_OF_SEQUENCES * SEQUENCE_LENGTH\n",
    "TARGET_TOTAL_LENGTH = SEQUENCE_LENGTH\n",
    "\n",
    "train_sub_generators = [MelodyTargetSequenceGenerator(\n",
    "                        PIANO_ROLLS_TRAIN[i*DATA_SIZE_OF_GENERATORS:(i+1)*DATA_SIZE_OF_GENERATORS], \n",
    "                        SEQUENCE_LENGTH, MELODY_NUMBER_OF_SEQUENCES, SEQUENCE_LENGTH, seed=i)\n",
    "                        \n",
    "                        for i in range(1 + PIANO_ROLLS_TRAIN.shape[0] // DATA_SIZE_OF_GENERATORS)\n",
    "                        if i*DATA_SIZE_OF_GENERATORS < PIANO_ROLLS_TRAIN.shape[0]]\n",
    "\n",
    "test_sub_generators = [MelodyTargetSequenceGenerator(\n",
    "                       PIANO_ROLLS_TEST[i*DATA_SIZE_OF_GENERATORS:(i+1)*DATA_SIZE_OF_GENERATORS], \n",
    "                       SEQUENCE_LENGTH, MELODY_NUMBER_OF_SEQUENCES, SEQUENCE_LENGTH, seed=i)\n",
    "                       \n",
    "                       for i in range(1 + PIANO_ROLLS_TEST.shape[0] // DATA_SIZE_OF_GENERATORS)\n",
    "                       if i*DATA_SIZE_OF_GENERATORS < PIANO_ROLLS_TEST.shape[0]]\n",
    "\n",
    "melody_gen_output_signature_roll = (tf.TensorSpec(shape=(128, SEQUENCE_LENGTH)),\n",
    "                                 tf.TensorSpec(shape=(128, SEQUENCE_LENGTH)))\n",
    "\n",
    "melody_gen_output_signature = melody_gen_output_signature_roll\n",
    "\n",
    "def get_sub_dataset(sub_generator, spec, batch_size, prefetch_size):\n",
    "    \n",
    "    return (tf.data.Dataset\n",
    "            .from_generator(sub_generator, output_signature=spec)\n",
    "            .batch(batch_size, drop_remainder=True)\n",
    "            .prefetch(prefetch_size)\n",
    "           )\n",
    "\n",
    "melody_train_sub_datasets = [get_sub_dataset(g, melody_gen_output_signature, BATCH_SIZE, 5)\n",
    "                          for g in train_sub_generators]\n",
    "melody_test_sub_datasets = [get_sub_dataset(g, melody_gen_output_signature, BATCH_SIZE, 5)\n",
    "                         for g in test_sub_generators]\n",
    "\n",
    "melody_train_dataset = tf.data.Dataset.sample_from_datasets(melody_train_sub_datasets).prefetch(64)\n",
    "melody_test_dataset = tf.data.Dataset.sample_from_datasets(melody_test_sub_datasets).prefetch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eff1ba70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 883 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sample_input = None\n",
    "for x in melody_train_dataset.take(1):\n",
    "    sample_input = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "704efb97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(32, 128, 32), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(32, 128, 32), dtype=tf.float32, name=None))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "melody_train_dataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "783fdc8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112.99999910593033"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_input[0].numpy().max()*127."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dcad27",
   "metadata": {},
   "source": [
    "## Creating and training melody predictor model\n",
    "\n",
    "To-do:<br>\n",
    "#1. Compare computing loss via latent vector MSE vs cross-entropy between input target pianoroll and decoded predicted latent vectors <br>\n",
    "#2. Consider creating model which learns autoencoding next target piano roll given melody"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff8f1f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handles encoding of sequences into latent vectors\n",
    "class VAEMelodyPredictior(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, latent_dim, batch_size,\n",
    "                 sequence_length, melody_number_of_sequences, \n",
    "                 encoder, decoder):\n",
    "        super(VAEMelodyPredictior, self).__init__()\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.sequence_length = sequence_length\n",
    "        self.melody_number_of_sequences = melody_number_of_sequences\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        #self.cvae = cvae\n",
    "        \n",
    "        # Set encoder to non-trainable\n",
    "        for l, _ in enumerate(self.encoder.layers):\n",
    "            self.encoder.get_layer(index=l).trainable = False\n",
    "        \n",
    "        # Set decoder to non-trainable\n",
    "        for l, _ in enumerate(self.decoder.layers):\n",
    "            self.decoder.get_layer(index=l).trainable = False\n",
    "            \n",
    "        # Set cvae to non-trainable\n",
    "        #for l, _ in enumerate(self.cvae.layers):\n",
    "        #    self.cvae.get_layer(index=l).trainable = False\n",
    "            \n",
    "        #rnn_input = Input(shape=(self.melody_number_of_sequences, self.latent_dim), name='rnn_input')\n",
    "        \n",
    "        #rnn_lstm_1 = LSTM(128, activation='relu', return_sequences=True, name='rnn_lstm_1')(rnn_input)\n",
    "        #rnn_lstm_2 = LSTM(128, activation='relu', name='rnn_lstm_2')(rnn_lstm_1)\n",
    "        \n",
    "        #rnn_dense_1 = Dense(128, activation='relu', name='rnn_dense_1')(rnn_lstm_2)\n",
    "        \n",
    "        #rnn_output = Dense(self.latent_dim, activation='relu', name='rnn_output')(rnn_dense_1)\n",
    "        \n",
    "        # MLP Definition\n",
    "        #mlp_input = Input(shape=(self.melody_number_of_sequences, self.latent_dim), name='dense_input')\n",
    "        mlp_input = Input(shape=(self.latent_dim), name='dense_input')\n",
    "        \n",
    "        mlp_flatten = Flatten(name='mlp_flatten')(mlp_input)\n",
    "        \n",
    "        mlp_dense_1 = Dense(256, activation='relu', kernel_regularizer='l2', name='dense_1')(mlp_flatten)\n",
    "        #mlp_dropout_1 = Dropout(.2, name='dropout_1')(mlp_dense_1)\n",
    "        \n",
    "        mlp_dense_2 = Dense(256, activation='relu', kernel_regularizer='l2', name='dense_2')(mlp_dense_1)\n",
    "        #mlp_dropout_2 = Dropout(.2, name='dropout_2')(mlp_dense_2)\n",
    "        \n",
    "        mlp_dense_3 = Dense(256, activation='relu', kernel_regularizer='l2', name='dense_3')(mlp_dense_2)\n",
    "        #mlp_dropout_3 = Dropout(.5, name='dropout_3')(mlp_dense_3)\n",
    "        \n",
    "        mlp_dense_4 = Dense(256, activation='relu', kernel_regularizer='l2', name='dense_4')(mlp_dense_3)\n",
    "        mlp_dropout_4 = Dropout(.2, name='mlp_dropout_4')(mlp_dense_3)\n",
    "        \n",
    "        mlp_output = Dense(self.latent_dim, activation='linear', name='mlp_output')(mlp_dense_4)\n",
    "        \n",
    "        # Model declaration\n",
    "        self.model = Model(mlp_input, mlp_output, name='model')\n",
    "        \n",
    "    def compile(self, optimizer):\n",
    "        super(VAEMelodyPredictior, self).compile()\n",
    "        \n",
    "        self.optimizer = optimizer\n",
    "        self.model.compile(optimizer=self.optimizer)\n",
    "        \n",
    "    def melody_to_stacked_tensor(self, x):\n",
    "        \n",
    "        # x has shape 128 x time_steps\n",
    "        \n",
    "        x_truncate = x.shape[-1] - x.shape[-1] % self.sequence_length\n",
    "        x_num_splits = x[:, :x_truncate].shape[-1] // self.sequence_length\n",
    "        \n",
    "        x_split = tf.split(x[:x_truncate], num_or_size_splits=x_num_splits)\n",
    "        return tf.stack(x_split)\n",
    "        \n",
    "    def encode_batch(self, B):\n",
    "        \n",
    "        # B has shape batch_size x melody_num_sequences x 128 x sequence_length\n",
    "        \n",
    "        #obs = [self.encoder(B[b])[0] for b in range(B.shape[0])] # <-- using CVAE class\n",
    "        obs = [self.encoder(B[b]) for b in range(B.shape[0])] # <-- using standalone encoder/decoder\n",
    "        \n",
    "        # returns tensor shaped batch_size x melody_num_sequences x latent_dim\n",
    "        return tf.stack(obs)\n",
    "    \n",
    "    def encode_melody(self, melody):\n",
    "        \n",
    "        # melody has shape melody_num_sequences x 128 x sequence_length\n",
    "        latent_vectors = [self.encoder(tf.expand_dims(melody[m], -1)) for m in range(m.shape[0])]\n",
    "        return tf.concat(latent_vectors, axis=0)\n",
    "        \n",
    "    def compute_loss(self, x_y):\n",
    "        \n",
    "        x, y = x_y\n",
    "        \n",
    "        x_latent_vectors = self.encoder(x)\n",
    "        y_latent_vectors = self.encoder(y)\n",
    "        \n",
    "        y_latent_vectors_pred = self.model(x_latent_vectors)\n",
    "        \n",
    "        # Reconstruction loss between piano rolls\n",
    "        #cross_entropy = K.binary_crossentropy(target=y, output=y_pred)    \n",
    "        #return K.sum(cross_entropy, axis=[1, 2])\n",
    "        \n",
    "        # MSE between latent vectors\n",
    "        return tf.keras.losses.MeanSquaredError()(y_latent_vectors, y_latent_vectors_pred)\n",
    "        \n",
    "    @tf.function\n",
    "    def train_step(self, x_y):\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self.compute_loss(x_y)\n",
    "        \n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        return {'mse': loss}\n",
    "    \n",
    "    @tf.function\n",
    "    def test_step(self, x_y):\n",
    "        \n",
    "        loss = self.compute_loss(x_y)\n",
    "        return {'mse': loss}\n",
    "    \n",
    "    def call(self, inputs, is_training=False):\n",
    "        \n",
    "        inputs_is_list = isinstance(inputs, list)\n",
    "        \n",
    "        if inputs_is_list and is_training:\n",
    "            return [self.train_step(x_y) for x_y in inputs]\n",
    "        \n",
    "        elif inputs_is_list and not is_training:\n",
    "            return [self.test_step(x_y) for x_y in inputs]\n",
    "        \n",
    "        elif not inputs_is_list and is_training:\n",
    "            return self.train_step(x_y)\n",
    "        \n",
    "        elif not inputs_is_list and not is_training:\n",
    "            return self.test_step(x_y)\n",
    "        \n",
    "    def predict_piano_roll(self, x_piano_rolls):\n",
    "        \n",
    "        x_latent = self.encoder(x_piano_rolls)\n",
    "            \n",
    "        y_pred_latent = self.model(x_latent)\n",
    "        \n",
    "        piano_roll_pred = self.decoder(y_pred_latent).numpy() * 127.\n",
    "        piano_roll_pred[piano_roll_pred > 127] = 127\n",
    "        piano_roll_pred[piano_roll_pred < 0] = 0\n",
    "        return piano_roll_pred.round().astype('uint8')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "54ff71dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "melody_vae = VAEMelodyPredictior(LATENT_DIM, BATCH_SIZE, SEQUENCE_LENGTH, MELODY_NUMBER_OF_SEQUENCES, encoder, decoder)\n",
    "\n",
    "#melody_vae = VAEMelodyPredictior(LATENT_DIM, BATCH_SIZE, \n",
    "#                                 SEQUENCE_LENGTH, MELODY_NUMBER_OF_SEQUENCES, \n",
    "#                                 cvae.encoder, cvae.decoder)\n",
    "\n",
    "melody_vae.compile(tf.keras.optimizers.Adam(1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "986c4f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "MELODY_VAE_MODEL_DIR = 'C:/_local/py/yt_piano_music_gen/models/melody_predictor/seq_32_z_64_mel_1_epochs_5_mse_0.016'\n",
    "\n",
    "melody_vae.model = tf.keras.models.load_model(MELODY_VAE_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "20efc6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_input (InputLayer)    [(None, 128)]             0         \n",
      "                                                                 \n",
      " mlp_flatten (Flatten)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               33024     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " mlp_output (Dense)          (None, 128)               32896     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 263,296\n",
      "Trainable params: 263,296\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "melody_vae.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d0d52acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MelodyVaeCheckpointCallback(tf.keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, model_dir):\n",
    "        \n",
    "        self.model_dir = model_dir        \n",
    "        self.best_loss = np.Inf\n",
    "        \n",
    "    def on_epoch_end(self, epoch, loss_dict):\n",
    "                \n",
    "        if loss_dict['mse'] < self.best_loss:\n",
    "            \n",
    "            self.latent_dim = self.model.latent_dim\n",
    "            self.seq_length = self.model.sequence_length\n",
    "            self.num_mel = self.model.melody_number_of_sequences\n",
    "            \n",
    "            self.best_epoch = epoch\n",
    "            self.best_loss = loss_dict['mse']   \n",
    "            \n",
    "            self.best_model = self.model.model\n",
    "    \n",
    "    def on_train_end(self, loss_dict):\n",
    "        \n",
    "        model_file_name = (f'seq_{self.seq_length}_z_{self.latent_dim}_mel_{self.num_mel}_'\n",
    "                           f'epochs_{self.best_epoch}_mse_{round(self.best_loss, 3)}'\n",
    "                          )\n",
    "        model_file_path = self.model_dir + model_file_name\n",
    "        \n",
    "        self.best_model.save(model_file_path)   \n",
    "\n",
    "melody_vae_ckpt_clbk = MelodyVaeCheckpointCallback(MELODY_PREDICTION_MODELS_DIR)\n",
    "melody_vae_reduce_lr_clbk = tf.keras.callbacks.ReduceLROnPlateau(monitor='mse', patience=3, min_delta=.0005, factor=.1)\n",
    "melody_vae_early_stop_clbk = tf.keras.callbacks.EarlyStopping(monitor='mse', patience=6, min_delta=.0005)\n",
    "\n",
    "melody_vae_callbacks = [melody_vae_ckpt_clbk, \n",
    "                        melody_vae_reduce_lr_clbk, \n",
    "                        melody_vae_early_stop_clbk\n",
    "                       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ebcca002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2048/2048 [==============================] - 187s 91ms/step - mse: 0.0285 - val_mse: 0.0468 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "2048/2048 [==============================] - 189s 92ms/step - mse: 0.0288 - val_mse: 0.0327 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "2048/2048 [==============================] - 191s 93ms/step - mse: 0.0285 - val_mse: 0.0302 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "2048/2048 [==============================] - 209s 102ms/step - mse: 0.0285 - val_mse: 0.0219 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "2048/2048 [==============================] - 227s 111ms/step - mse: 0.0282 - val_mse: 0.0305 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "2048/2048 [==============================] - 237s 116ms/step - mse: 0.0281 - val_mse: 0.0192 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "2048/2048 [==============================] - 244s 119ms/step - mse: 0.0280 - val_mse: 0.0239 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "2048/2048 [==============================] - 242s 118ms/step - mse: 0.0280 - val_mse: 0.0231 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "2048/2048 [==============================] - 241s 118ms/step - mse: 0.0273 - val_mse: 0.0381 - lr: 1.0000e-04\n",
      "Epoch 10/10\n",
      "2048/2048 [==============================] - 255s 125ms/step - mse: 0.0272 - val_mse: 0.0193 - lr: 1.0000e-04\n",
      "INFO:tensorflow:Assets written to: C:/_local/py/yt_piano_music_gen/models/melody_predictor/seq_32_z_128_mel_1_epochs_4_mse_0.015\\assets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e3b70892b0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "melody_vae.fit(x=melody_train_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "               epochs=10, steps_per_epoch=2048,\n",
    "               validation_data=melody_test_dataset, validation_steps=256,\n",
    "               callbacks=melody_vae_callbacks\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d8055f",
   "metadata": {},
   "source": [
    "## Generating music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7d0ddec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_output_batch = melody_vae.predict_piano_roll(sample_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f36cf0ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 128, 32)\n",
      "114\n"
     ]
    }
   ],
   "source": [
    "print(sample_output_batch.shape)\n",
    "print(sample_output_batch.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c57d5eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample # 29\n"
     ]
    }
   ],
   "source": [
    "play_samples_from_batch(sample_output_batch, 15, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7c3044fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_piano_roll = sample_input[0][7].numpy()*127.\n",
    "sample_piano_roll = sample_piano_roll.round()\n",
    "#sample_piano_roll = np.concatenate([sample_piano_roll[s] for s in range(sample_piano_roll.shape[0])], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "75737a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_output = melody_vae.predict_piano_roll(np.expand_dims(sample_input[0][6], 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6414e92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 32)\n",
      "93.0\n"
     ]
    }
   ],
   "source": [
    "print(sample_piano_roll.shape)\n",
    "print(sample_piano_roll.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c57498f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 128, 32)\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(sample_output.shape)\n",
    "print(sample_output.min())\n",
    "print(sample_output.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "085aeeeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 503 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "play_piano_roll(sample_piano_roll, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "37ff9d35",
   "metadata": {},
   "outputs": [
    {
     "ename": "UFuncTypeError",
     "evalue": "Cannot cast ufunc 'multiply' output from dtype('float64') to dtype('uint8') with casting rule 'same_kind'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUFuncTypeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_78548/2910724957.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplay_piano_roll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_78548/3547796692.py\u001b[0m in \u001b[0;36mplay_piano_roll\u001b[1;34m(piano_roll, buffer_time, threshold, temp_path)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mpiano_roll\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1.5\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mpiano_roll\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[1;36m127.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mpiano_roll\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpiano_roll\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m127\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m126\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUFuncTypeError\u001b[0m: Cannot cast ufunc 'multiply' output from dtype('float64') to dtype('uint8') with casting rule 'same_kind'"
     ]
    }
   ],
   "source": [
    "play_piano_roll(sample_output.squeeze(), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "58603cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_song = np.concatenate([sample_piano_roll, sample_output.squeeze()], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "467ecad5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 64)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_song.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "bbc4bc14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 826 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "play_piano_roll(sample_song, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "5b60d595",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_song_from_input(input_array, model, sequence_length, number_of_sequences):\n",
    "    \n",
    "    # input_array is 1 x M x 128 x sequence_length with elements in [0, 127]\n",
    "    \n",
    "    if input_array.shape[-1] != sequence_length or input_array.shape[-2] != 128:\n",
    "        print('bad input shape: bad sequence_length or number of pitches')\n",
    "        return\n",
    "    \n",
    "    if input_array.max() <= 1.5:\n",
    "        input_array *= 127.\n",
    "    \n",
    "    input_array[input_array > 127] = 127\n",
    "    input_array[input_array < 0] = 0\n",
    "    input_array = input_array.round().astype('uint8')\n",
    "           \n",
    "    total_number_of_sequences = input_array.shape[0] + number_of_sequences\n",
    "    \n",
    "    piano_roll = np.zeros(shape=(total_number_of_sequences, 128, sequence_length))\n",
    "    \n",
    "    x_start = 0\n",
    "    x_end = input_array.shape[0]\n",
    "    piano_roll[x_start:x_end, :, :] = input_array\n",
    "    \n",
    "    y_start = x_end\n",
    "    y_end = y_start + 1\n",
    "    \n",
    "    for n in range(number_of_sequences):\n",
    "        \n",
    "        x = piano_roll[x_start:x_end, :, :]          \n",
    "        y = model.predict_piano_roll(x).squeeze()\n",
    "        \n",
    "        piano_roll[y_start:y_end, :] = y\n",
    "        \n",
    "        x_start += 1\n",
    "        x_end += 1\n",
    "        \n",
    "        y_start += 1\n",
    "        y_end += 1\n",
    "        \n",
    "    piano_roll_list = [piano_roll[s] for s in range(piano_roll.shape[0])]\n",
    "    return np.concatenate(piano_roll_list, axis=-1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "a81e833e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (128,128,32) into shape (1,128,32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_69888/1287547017.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msample_song\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_song_from_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_input\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmelody_vae\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSEQUENCE_LENGTH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_69888/1014796095.py\u001b[0m in \u001b[0;36mgenerate_song_from_input\u001b[1;34m(input_array, model, sequence_length, number_of_sequences)\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_piano_roll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0mpiano_roll\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my_start\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0my_end\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mx_start\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not broadcast input array from shape (128,128,32) into shape (1,128,32)"
     ]
    }
   ],
   "source": [
    "sample_song = generate_song_from_input(sample_input[0][5].numpy(), melody_vae, SEQUENCE_LENGTH, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "3589cf00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 416)\n",
      "127.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(sample_song.shape)\n",
    "print(sample_song.max())\n",
    "print(sample_song.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "eada313b",
   "metadata": {},
   "outputs": [],
   "source": [
    "play_piano_roll(sample_song)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3be3456",
   "metadata": {},
   "source": [
    "## VAE with melody prediction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "274bd4d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(32, 1, 128, 32) dtype=float32 (created by layer 'model_input')>"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input = Input(batch_shape=rnn_train_dataset.element_spec[0].shape, name='model_input')\n",
    "model_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "0c2cceeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder definition\n",
    "encoder_input = Reshape(target_shape=model_input.shape[1:]+(1,), name='encoder_input_reshape')(model_input)\n",
    "\n",
    "encoder_conv_1 = Conv3D(filters=64, kernel_size=(1, 4, 4), strides=(1, 4, 4), \n",
    "                        activation='relu',name='encoder_conv2d_1')(encoder_input)\n",
    "\n",
    "encoder_conv_2 = Conv3D(filters=128, kernel_size=(1, 4, 4), strides=(1, 4, 4), \n",
    "                        activation='relu', name='encoder_conv2d_2')(encoder_conv_1)\n",
    "\n",
    "encoder_conv_3 = Conv3D(filters=256, kernel_size=(1, 8, 2), strides=(1, 8, 2), \n",
    "                        activation='linear', name='encoder_conv2d_3')(encoder_conv_2)\n",
    "\n",
    "encoder_batch_norm = BatchNorm(name='encoder_batch_norm_3')(encoder_conv_3)\n",
    "encoder_relu_3 = Relu(name='encoder_relu_3')(encoder_batch_norm)\n",
    "\n",
    "encoder_flatten = Flatten(name='encoder_flatten')(encoder_relu_3)\n",
    "\n",
    "encoder_dense_1 = Dense(128, activation='relu', name='encoder_dense_1')(encoder_flatten)\n",
    "\n",
    "encoder_mu = Dense(LATENT_DIM, activation='linear', name='encoder_mu')(encoder_dense_1)\n",
    "encoder_log_sigma = Dense(LATENT_DIM, activation='linear', name='variance')(encoder_dense_1)\n",
    "\n",
    "encoder = Model(encoder_input, encoder_mu, name='encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "dcaa5fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(32, 1, 128, 32, 1)]     0         \n",
      "                                                                 \n",
      " encoder_conv2d_1 (Conv3D)   (32, 1, 32, 8, 64)        1088      \n",
      "                                                                 \n",
      " encoder_conv2d_2 (Conv3D)   (32, 1, 8, 2, 128)        131200    \n",
      "                                                                 \n",
      " encoder_conv2d_3 (Conv3D)   (32, 1, 1, 1, 256)        524544    \n",
      "                                                                 \n",
      " encoder_batch_norm_3 (Batch  (32, 1, 1, 1, 256)       1024      \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " encoder_relu_3 (ReLU)       (32, 1, 1, 1, 256)        0         \n",
      "                                                                 \n",
      " encoder_flatten (Flatten)   (32, 256)                 0         \n",
      "                                                                 \n",
      " encoder_dense_1 (Dense)     (32, 128)                 32896     \n",
      "                                                                 \n",
      " encoder_mu (Dense)          (32, 128)                 16512     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 707,264\n",
      "Trainable params: 706,752\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "51f4e5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reparameterization and concatenation layers / specification\n",
    "def sample_and_reparameterize(mu_logsigma):\n",
    "    mu, log_sigma = mu_logsigma\n",
    "    eps = K.random_normal(shape=(LATENT_DIM,))\n",
    "    return mu + K.exp(log_sigma * .5) * eps\n",
    "\n",
    "Z = Lambda(sample_and_reparameterize, output_shape=(LATENT_DIM,), \n",
    "           name='sample_and_reparameterize')([encoder_mu, encoder_log_sigma])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "516d0bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melody model definition\n",
    "\n",
    "melody_dense_1_layer = Dense(128, activation='relu', name='melody_dense_1')\n",
    "melody_dense_2_layer = Dense(128, activation='relu', name='melody_dense_2')\n",
    "melody_dense_3_layer = Dense(128, activation='linear', name='melody_dense_3')\n",
    "\n",
    "m1 = melody_dense_1_layer(Z)\n",
    "m2 = melody_dense_2_layer(m1)\n",
    "melody_output = melody_dense_3_layer(m2)\n",
    "\n",
    "melody_input = Input(batch_shape=(BATCH_SIZE, LATENT_DIM), name='melody_input')\n",
    "melody_dense_1 = melody_dense_1_layer(melody_input)\n",
    "melody_dense_2 = melody_dense_2_layer(melody_dense_1)\n",
    "melody_dense_3 = melody_dense_3_layer(melody_dense_2)\n",
    "\n",
    "melody_model = Model(melody_input, melody_dense_3, name='melody_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "c95295da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"melody_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " melody_input (InputLayer)   [(32, 128)]               0         \n",
      "                                                                 \n",
      " melody_dense_1 (Dense)      (32, 128)                 16512     \n",
      "                                                                 \n",
      " melody_dense_2 (Dense)      (32, 128)                 16512     \n",
      "                                                                 \n",
      " melody_dense_3 (Dense)      (32, 128)                 16512     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 49,536\n",
      "Trainable params: 49,536\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "melody_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "eb4f0cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder definition\n",
    "\n",
    "decoder_dense_1_layer = Dense(128, activation='relu', name='decoder_dense_1')\n",
    "decoder_dense_2_layer = Dense(128, activation='relu', name='decoder_dense_2')\n",
    "decoder_reshape_1_layer = Reshape(target_shape=(1, 1, 128), name='reshape')\n",
    "\n",
    "decoder_conv_1_layer = Conv2DTranspose(filters=128, kernel_size=(8, 2), strides=(8, 2), \n",
    "                                       activation='relu', padding='same', \n",
    "                                       name='conv2dtranspose_1')\n",
    "\n",
    "decoder_conv_2_layer = Conv2DTranspose(filters=64, kernel_size=(4, 4), strides=(4, 4), \n",
    "                                       activation='relu', padding='same', \n",
    "                                       name='conv2dtranspose_2')\n",
    "\n",
    "decoder_conv_3_layer = Conv2DTranspose(filters=1, kernel_size=(4, 4), strides=(4, 4), \n",
    "                                       activation='relu', padding='same', name='conv2dtranspose_3')\n",
    "\n",
    "decoder_output_layer = Reshape(target_shape=(128, SEQUENCE_LENGTH), name='decoder_output')\n",
    "\n",
    "d1 = decoder_dense_1_layer(melody_dense_3)\n",
    "d2 = decoder_dense_2_layer(d1)\n",
    "d3 = decoder_reshape_1_layer(d2)\n",
    "d4 = decoder_conv_1_layer(d3)\n",
    "d5 = decoder_conv_2_layer(d4)\n",
    "d6 = decoder_conv_3_layer(d5)\n",
    "melody_cvae_output = decoder_output_layer(d6)\n",
    "\n",
    "decoder_input = Input(shape=(LATENT_DIM,), name='decoder_input')\n",
    "decoder_dense_1 = decoder_dense_1_layer(decoder_input)\n",
    "decoder_dense_2 = decoder_dense_2_layer(decoder_dense_1)\n",
    "decoder_reshape_1 = decoder_reshape_1_layer(decoder_dense_2)\n",
    "decoder_conv_1 = decoder_conv_1_layer(decoder_reshape_1)\n",
    "decoder_conv_2 = decoder_conv_2_layer(decoder_conv_1)\n",
    "decoder_conv_3 = decoder_conv_3_layer(decoder_conv_2)\n",
    "decoder_output = decoder_output_layer(decoder_conv_3)\n",
    "\n",
    "decoder = Model(decoder_input, decoder_output, name='decoder')\n",
    "\n",
    "melody_cvae = Model(model_input, cvae_output, name='cvae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "cbb5aa6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 128)]             0         \n",
      "                                                                 \n",
      " decoder_dense_1 (Dense)     multiple                  16512     \n",
      "                                                                 \n",
      " decoder_dense_2 (Dense)     multiple                  16512     \n",
      "                                                                 \n",
      " reshape (Reshape)           multiple                  0         \n",
      "                                                                 \n",
      " conv2dtranspose_1 (Conv2DTr  multiple                 262272    \n",
      " anspose)                                                        \n",
      "                                                                 \n",
      " conv2dtranspose_2 (Conv2DTr  multiple                 131136    \n",
      " anspose)                                                        \n",
      "                                                                 \n",
      " conv2dtranspose_3 (Conv2DTr  multiple                 1025      \n",
      " anspose)                                                        \n",
      "                                                                 \n",
      " decoder_output (Reshape)    multiple                  0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 427,457\n",
      "Trainable params: 427,457\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "00fbb470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 128, 32) dtype=float32 (created by layer 'decoder_output')>"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "67be357b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"cvae\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " model_input (InputLayer)       [(32, 1, 128, 32)]   0           []                               \n",
      "                                                                                                  \n",
      " encoder_input_reshape (Reshape  (32, 1, 128, 32, 1)  0          ['model_input[0][0]']            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " encoder_conv2d_1 (Conv3D)      (32, 1, 32, 8, 64)   1088        ['encoder_input_reshape[0][0]']  \n",
      "                                                                                                  \n",
      " encoder_conv2d_2 (Conv3D)      (32, 1, 8, 2, 128)   131200      ['encoder_conv2d_1[0][0]']       \n",
      "                                                                                                  \n",
      " encoder_conv2d_3 (Conv3D)      (32, 1, 1, 1, 256)   524544      ['encoder_conv2d_2[0][0]']       \n",
      "                                                                                                  \n",
      " encoder_batch_norm_3 (BatchNor  (32, 1, 1, 1, 256)  1024        ['encoder_conv2d_3[0][0]']       \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " encoder_relu_3 (ReLU)          (32, 1, 1, 1, 256)   0           ['encoder_batch_norm_3[0][0]']   \n",
      "                                                                                                  \n",
      " encoder_flatten (Flatten)      (32, 256)            0           ['encoder_relu_3[0][0]']         \n",
      "                                                                                                  \n",
      " encoder_dense_1 (Dense)        (32, 128)            32896       ['encoder_flatten[0][0]']        \n",
      "                                                                                                  \n",
      " encoder_mu (Dense)             (32, 128)            16512       ['encoder_dense_1[0][0]']        \n",
      "                                                                                                  \n",
      " variance (Dense)               (32, 128)            16512       ['encoder_dense_1[0][0]']        \n",
      "                                                                                                  \n",
      " sample_and_reparameterize (Lam  (32, 128)           0           ['encoder_mu[0][0]',             \n",
      " bda)                                                             'variance[0][0]']               \n",
      "                                                                                                  \n",
      " decoder_dense_1 (Dense)        multiple             16512       ['sample_and_reparameterize[0][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " decoder_dense_2 (Dense)        multiple             16512       ['decoder_dense_1[0][0]']        \n",
      "                                                                                                  \n",
      " reshape (Reshape)              multiple             0           ['decoder_dense_2[0][0]']        \n",
      "                                                                                                  \n",
      " conv2dtranspose_1 (Conv2DTrans  multiple            262272      ['reshape[0][0]']                \n",
      " pose)                                                                                            \n",
      "                                                                                                  \n",
      " conv2dtranspose_2 (Conv2DTrans  multiple            131136      ['conv2dtranspose_1[0][0]']      \n",
      " pose)                                                                                            \n",
      "                                                                                                  \n",
      " conv2dtranspose_3 (Conv2DTrans  multiple            1025        ['conv2dtranspose_2[0][0]']      \n",
      " pose)                                                                                            \n",
      "                                                                                                  \n",
      " decoder_output (Reshape)       multiple             0           ['conv2dtranspose_3[0][0]']      \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,151,233\n",
      "Trainable params: 1,150,721\n",
      "Non-trainable params: 512\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "melody_cvae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c21da1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "488cde0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, latent_dim, sequence_length, batch_size):\n",
    "        super(CVAE, self).__init__()\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.sequence_length = sequence_length\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Encoder definition\n",
    "        self.X = Input(shape=(128, self.sequence_length), batch_size=self.batch_size, name='input')\n",
    "        X_add_channel = Reshape(target_shape=(128, self.sequence_length, 1), name='encoder_input_reshape')(self.X)\n",
    "\n",
    "        encoder_conv_1 = Conv2D(filters=64, kernel_size=(4, 4), strides=(4, 4), \n",
    "                                activation='relu', padding='valid', name='conv2d_1')(X_add_channel)\n",
    "\n",
    "        #encoder_relu_1 = Relu(name='encoder_relu_1')(encoder_batch_norm_1)\n",
    "        #encoder_batch_norm_1 = BatchNorm(name='encoder_batch_norm_1')(encoder_conv_1)\n",
    "\n",
    "        encoder_conv_2 = Conv2D(filters=128, kernel_size=(4, 4), strides=(4, 4), \n",
    "                                activation='relu', padding='valid', name='conv2d_2')(encoder_conv_1)\n",
    "\n",
    "        #encoder_batch_norm_2 = BatchNorm(name='encoder_batch_norm_2')(encoder_conv_2)\n",
    "        #encoder_relu_2 = Relu(name='encoder_relu_2')(encoder_batch_norm_2)\n",
    "\n",
    "        encoder_conv_3 = Conv2D(filters=256, kernel_size=(8, 2), strides=(8, 2), \n",
    "                                activation='relu', padding='valid', name='conv2d_3')(encoder_conv_2)\n",
    "\n",
    "        #encoder_batch_norm_3 = BatchNorm(name='encoder_batch_norm_3')(encoder_conv_3)\n",
    "        #encoder_relu_3 = Relu(name='encoder_relu_3')(encoder_batch_norm_3)\n",
    "\n",
    "        encoder_flatten = Flatten(name='encoder_flatten')(encoder_conv_3)\n",
    "\n",
    "        #encoder_dense_1 = Dense(256, activation='relu', name='encoder_dense_1')(encoder_flatten)\n",
    "        #encoder_dense_2 = Dense(256, activation='relu', name='encoder_dense_2')(encoder_dense_1)\n",
    "        #encoder_dense_3 = Dense(256, activation='relu', name='encoder_dense_3')(encoder_dense_2)\n",
    "\n",
    "        self.encoder_mu = Dense(self.latent_dim, activation='linear', name='encoder_mu')(encoder_flatten)\n",
    "        self.encoder_log_sigma = Dense(self.latent_dim, activation='linear', name='encoder_log_sigma')(encoder_flatten)\n",
    "\n",
    "        self.encoder = Model(self.X, self.encoder_mu, name='encoder')\n",
    "        \n",
    "        # Sample and reparameterize layer\n",
    "        Z = Lambda(self.sample_and_reparameterize, output_shape=(self.latent_dim,), batch_size=self.batch_size,\n",
    "                   name='sample_and_reparameterize')([self.encoder_mu, self.encoder_log_sigma])\n",
    "        \n",
    "        # Decoder definition\n",
    "        decoder_dense_1_layer = Dense(256, activation='relu', name='decoder_dense_1')\n",
    "        #decoder_dense_2_layer = Dense(256, activation='relu', name='decoder_dense_2')\n",
    "        #decoder_dense_3_layer = Dense(256, activation='relu', name='decoder_dense_3')\n",
    "        decoder_reshape_1_layer = Reshape(target_shape=(1, 1, 256), name='reshape')\n",
    "\n",
    "        decoder_conv_1_layer = Conv2DTranspose(filters=128, kernel_size=(8, 2), strides=(8, 2), \n",
    "                                               activation='relu', padding='valid', \n",
    "                                               name='conv2dtranspose_1')\n",
    "\n",
    "        decoder_conv_2_layer = Conv2DTranspose(filters=64, kernel_size=(4, 4), strides=(4, 4), \n",
    "                                               activation='relu', padding='valid', \n",
    "                                               name='conv2dtranspose_2')\n",
    "\n",
    "        decoder_conv_3_layer = Conv2DTranspose(filters=1, kernel_size=(4, 4), strides=(4, 4), \n",
    "                                               activation='sigmoid', padding='valid', \n",
    "                                               name='conv2dtranspose_3')\n",
    "\n",
    "        decoder_output_layer = Reshape(target_shape=(128, self.sequence_length), name='decoder_output')\n",
    "\n",
    "        d1 = decoder_dense_1_layer(Z)\n",
    "        #d2 = decoder_dense_2_layer(d1)\n",
    "        #d3 = decoder_dense_3_layer(d2)\n",
    "        d4 = decoder_reshape_1_layer(d1)\n",
    "        d5 = decoder_conv_1_layer(d4)\n",
    "        d6 = decoder_conv_2_layer(d5)\n",
    "        d7 = decoder_conv_3_layer(d6)\n",
    "        self.cvae_output = decoder_output_layer(d7)\n",
    "\n",
    "        decoder_input = Input(shape=(self.latent_dim,), batch_size=self.batch_size, name='decoder_input')\n",
    "        decoder_dense_1 = decoder_dense_1_layer(decoder_input)\n",
    "        #decoder_dense_2 = decoder_dense_2_layer(decoder_dense_1)\n",
    "        #decoder_dense_3 = decoder_dense_3_layer(decoder_dense_2)\n",
    "        decoder_reshape_1 = decoder_reshape_1_layer(decoder_dense_1)\n",
    "        decoder_conv_1 = decoder_conv_1_layer(decoder_reshape_1)\n",
    "        decoder_conv_2 = decoder_conv_2_layer(decoder_conv_1)\n",
    "        decoder_conv_3 = decoder_conv_3_layer(decoder_conv_2)\n",
    "        decoder_output = decoder_output_layer(decoder_conv_3)\n",
    "\n",
    "        self.decoder = Model(decoder_input, decoder_output, name='decoder')\n",
    "\n",
    "        self.cvae = Model(self.X, self.cvae_output, name='cvae')\n",
    "                  \n",
    "        self.cvae.add_loss(self.vae_loss(self.X, self.cvae_output, self.encoder_mu, self.encoder_log_sigma))\n",
    "        self.cvae.add_metric(self.reconstruction_loss(self.X, self.cvae_output), name='reconstruction_loss')\n",
    "        self.cvae.add_metric(self.kl_divergence(self.encoder_mu, self.encoder_log_sigma), name='kl_divergence')\n",
    "        \n",
    "    def compile(self, optimizer):\n",
    "        super(CVAE, self).compile()\n",
    "\n",
    "        self.encoder.compile(optimizer, loss=None)            \n",
    "        self.decoder.compile(optimizer, loss=None)\n",
    "        self.cvae.compile(optimizer, loss=None)            \n",
    "\n",
    "    def sample_and_reparameterize(self, mu_log_sigma):\n",
    "\n",
    "        mu, log_sigma = mu_log_sigma\n",
    "\n",
    "        eps = K.random_normal(shape=(self.latent_dim,))\n",
    "        return mu + K.exp(log_sigma * .5) * eps\n",
    "\n",
    "    def reconstruction_loss(self, x_true, x_pred):\n",
    "\n",
    "        cross_entropy = K.binary_crossentropy(target=x_true, output=x_pred)    \n",
    "        return K.sum(cross_entropy, axis=[1, 2])\n",
    "\n",
    "    def kl_divergence(self, emu, els):\n",
    "\n",
    "        kl_2 = K.exp(els) + K.square(emu) - 1. - els\n",
    "        kl = .5 * K.sum(kl_2, axis=-1)\n",
    "        return kl\n",
    "\n",
    "    def vae_loss(self, x_true, x_pred, emu, els):\n",
    "\n",
    "        recon_loss = self.reconstruction_loss(x_true, x_pred)\n",
    "        kl_loss = self.kl_divergence(emu, els)\n",
    "\n",
    "        return recon_loss + kl_loss\n",
    "    \n",
    "    def call(self, inputs, is_training=False):\n",
    "        \n",
    "        inputs_is_list = isinstance(inputs, list)\n",
    "        \n",
    "        if inputs_is_list and is_training:\n",
    "            return [self.train_step(x_y) for x_y in inputs]\n",
    "        \n",
    "        elif inputs_is_list and not is_training:\n",
    "            return [self.test_step(x_y) for x_y in inputs]\n",
    "        \n",
    "        elif not inputs_is_list and is_training:\n",
    "            return self.train_step(inputs)\n",
    "        \n",
    "        elif not inputs_is_list and not is_training:\n",
    "            return self.test_step(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "38f8e331",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvae = CVAE(LATENT_DIM, SEQUENCE_LENGTH, BATCH_SIZE)\n",
    "cvae.compile(tf.keras.optimizers.Adam(1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78d1fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvae.fit(x=cvae_train_dataset, shuffle=False,\n",
    "         batch_size=BATCH_SIZE, steps_per_epoch=128, \n",
    "         epochs=1,\n",
    "         validation_data=cvae_test_dataset, validation_steps=16,\n",
    "         #callbacks=vae_clbks\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf_env]",
   "language": "python",
   "name": "conda-env-tf_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
